# Comparing `tmp/xpag-0.1.9-py3-none-any.whl.zip` & `tmp/xpag-0.2.0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,42 +1,42 @@
-Zip file size: 54938 bytes, number of entries: 40
--rw-r--r--  2.0 unx      211 b- defN 23-May-16 09:12 xpag/__init__.py
--rw-r--r--  2.0 unx      204 b- defN 23-May-16 09:12 xpag/agents/__init__.py
--rw-r--r--  2.0 unx     1098 b- defN 23-May-16 09:12 xpag/agents/agent.py
--rw-r--r--  2.0 unx        0 b- defN 23-May-16 09:12 xpag/agents/sac/__init__.py
--rw-r--r--  2.0 unx     5554 b- defN 23-May-16 09:12 xpag/agents/sac/sac.py
--rw-r--r--  2.0 unx    18817 b- defN 23-May-16 09:12 xpag/agents/sac/sac_from_jaxrl.py
--rw-r--r--  2.0 unx        0 b- defN 23-May-16 09:12 xpag/agents/sdqn/__init__.py
--rw-r--r--  2.0 unx    22063 b- defN 23-May-16 09:12 xpag/agents/sdqn/sdqn.py
--rw-r--r--  2.0 unx        0 b- defN 23-May-16 09:12 xpag/agents/td3/__init__.py
--rw-r--r--  2.0 unx    15221 b- defN 23-May-16 09:12 xpag/agents/td3/td3.py
--rw-r--r--  2.0 unx        0 b- defN 23-May-16 09:12 xpag/agents/tqc/__init__.py
--rw-r--r--  2.0 unx    12988 b- defN 23-May-16 09:12 xpag/agents/tqc/tqc.py
--rw-r--r--  2.0 unx      160 b- defN 23-May-16 09:12 xpag/buffers/__init__.py
--rw-r--r--  2.0 unx    11221 b- defN 23-May-16 09:12 xpag/buffers/buffer.py
--rw-r--r--  2.0 unx     5236 b- defN 23-May-16 09:12 xpag/buffers/jax_buffer.py
--rw-r--r--  2.0 unx       55 b- defN 23-May-16 09:12 xpag/plotting/__init__.py
--rw-r--r--  2.0 unx     4086 b- defN 23-May-16 09:12 xpag/plotting/plotting.py
--rw-r--r--  2.0 unx     2841 b- defN 23-May-16 09:12 xpag/samplers/HER.py
--rw-r--r--  2.0 unx      182 b- defN 23-May-16 09:12 xpag/samplers/__init__.py
--rw-r--r--  2.0 unx     2941 b- defN 23-May-16 09:12 xpag/samplers/jax_sampler.py
--rw-r--r--  2.0 unx     1917 b- defN 23-May-16 09:12 xpag/samplers/sampler.py
--rw-r--r--  2.0 unx       71 b- defN 23-May-16 09:12 xpag/setters/__init__.py
--rw-r--r--  2.0 unx     3734 b- defN 23-May-16 09:12 xpag/setters/setter.py
--rw-r--r--  2.0 unx      438 b- defN 23-May-16 09:12 xpag/tools/__init__.py
--rw-r--r--  2.0 unx     5065 b- defN 23-May-16 09:12 xpag/tools/eval.py
--rw-r--r--  2.0 unx     7611 b- defN 23-May-16 09:12 xpag/tools/learn.py
--rw-r--r--  2.0 unx     3225 b- defN 23-May-16 09:12 xpag/tools/logging.py
--rw-r--r--  2.0 unx     4708 b- defN 23-May-16 09:12 xpag/tools/replay.py
--rw-r--r--  2.0 unx      899 b- defN 23-May-16 09:12 xpag/tools/timing.py
--rw-r--r--  2.0 unx     6933 b- defN 23-May-16 09:12 xpag/tools/utils.py
--rw-r--r--  2.0 unx      179 b- defN 23-May-16 09:12 xpag/wrappers/__init__.py
--rw-r--r--  2.0 unx     8257 b- defN 23-May-16 09:12 xpag/wrappers/brax_vec_env.py
--rw-r--r--  2.0 unx     6099 b- defN 23-May-16 09:12 xpag/wrappers/goalenv_wrapper.py
--rw-r--r--  2.0 unx    10148 b- defN 23-May-16 09:12 xpag/wrappers/gym_vec_env.py
--rw-r--r--  2.0 unx      936 b- defN 23-May-16 09:12 xpag/wrappers/reset_done.py
--rw-r--r--  2.0 unx     1517 b- defN 23-May-16 09:13 xpag-0.1.9.dist-info/LICENSE
--rw-r--r--  2.0 unx    15098 b- defN 23-May-16 09:13 xpag-0.1.9.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 23-May-16 09:13 xpag-0.1.9.dist-info/WHEEL
--rw-r--r--  2.0 unx        5 b- defN 23-May-16 09:13 xpag-0.1.9.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     3184 b- defN 23-May-16 09:13 xpag-0.1.9.dist-info/RECORD
-40 files, 182994 bytes uncompressed, 49916 bytes compressed:  72.7%
+Zip file size: 55026 bytes, number of entries: 40
+-rw-r--r--  2.0 unx      211 b- defN 23-Jul-12 09:50 xpag/__init__.py
+-rw-r--r--  2.0 unx      204 b- defN 23-Jul-12 09:50 xpag/agents/__init__.py
+-rw-r--r--  2.0 unx     1098 b- defN 23-Jul-12 09:50 xpag/agents/agent.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-12 09:50 xpag/agents/sac/__init__.py
+-rw-r--r--  2.0 unx     5594 b- defN 23-Jul-12 09:50 xpag/agents/sac/sac.py
+-rw-r--r--  2.0 unx    18793 b- defN 23-Jul-12 09:50 xpag/agents/sac/sac_from_jaxrl.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-12 09:50 xpag/agents/sdqn/__init__.py
+-rw-r--r--  2.0 unx    22289 b- defN 23-Jul-12 09:50 xpag/agents/sdqn/sdqn.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-12 09:50 xpag/agents/td3/__init__.py
+-rw-r--r--  2.0 unx    15288 b- defN 23-Jul-12 09:50 xpag/agents/td3/td3.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-12 09:50 xpag/agents/tqc/__init__.py
+-rw-r--r--  2.0 unx    13028 b- defN 23-Jul-12 09:50 xpag/agents/tqc/tqc.py
+-rw-r--r--  2.0 unx      160 b- defN 23-Jul-12 09:50 xpag/buffers/__init__.py
+-rw-r--r--  2.0 unx    11221 b- defN 23-Jul-12 09:50 xpag/buffers/buffer.py
+-rw-r--r--  2.0 unx     5236 b- defN 23-Jul-12 09:50 xpag/buffers/jax_buffer.py
+-rw-r--r--  2.0 unx       55 b- defN 23-Jul-12 09:50 xpag/plotting/__init__.py
+-rw-r--r--  2.0 unx     4086 b- defN 23-Jul-12 09:50 xpag/plotting/plotting.py
+-rw-r--r--  2.0 unx     2841 b- defN 23-Jul-12 09:50 xpag/samplers/HER.py
+-rw-r--r--  2.0 unx      182 b- defN 23-Jul-12 09:50 xpag/samplers/__init__.py
+-rw-r--r--  2.0 unx     2941 b- defN 23-Jul-12 09:50 xpag/samplers/jax_sampler.py
+-rw-r--r--  2.0 unx     1917 b- defN 23-Jul-12 09:50 xpag/samplers/sampler.py
+-rw-r--r--  2.0 unx       71 b- defN 23-Jul-12 09:50 xpag/setters/__init__.py
+-rw-r--r--  2.0 unx     3734 b- defN 23-Jul-12 09:50 xpag/setters/setter.py
+-rw-r--r--  2.0 unx      438 b- defN 23-Jul-12 09:50 xpag/tools/__init__.py
+-rw-r--r--  2.0 unx     5065 b- defN 23-Jul-12 09:50 xpag/tools/eval.py
+-rw-r--r--  2.0 unx     7603 b- defN 23-Jul-12 09:50 xpag/tools/learn.py
+-rw-r--r--  2.0 unx     3225 b- defN 23-Jul-12 09:50 xpag/tools/logging.py
+-rw-r--r--  2.0 unx     4708 b- defN 23-Jul-12 09:50 xpag/tools/replay.py
+-rw-r--r--  2.0 unx      899 b- defN 23-Jul-12 09:50 xpag/tools/timing.py
+-rw-r--r--  2.0 unx     6933 b- defN 23-Jul-12 09:50 xpag/tools/utils.py
+-rw-r--r--  2.0 unx      179 b- defN 23-Jul-12 09:50 xpag/wrappers/__init__.py
+-rw-r--r--  2.0 unx     8189 b- defN 23-Jul-12 09:50 xpag/wrappers/brax_vec_env.py
+-rw-r--r--  2.0 unx     6099 b- defN 23-Jul-12 09:50 xpag/wrappers/goalenv_wrapper.py
+-rw-r--r--  2.0 unx    10148 b- defN 23-Jul-12 09:50 xpag/wrappers/gym_vec_env.py
+-rw-r--r--  2.0 unx      936 b- defN 23-Jul-12 09:50 xpag/wrappers/reset_done.py
+-rw-r--r--  2.0 unx     1517 b- defN 23-Jul-12 09:50 xpag-0.2.0.dist-info/LICENSE
+-rw-r--r--  2.0 unx    15098 b- defN 23-Jul-12 09:50 xpag-0.2.0.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 23-Jul-12 09:50 xpag-0.2.0.dist-info/WHEEL
+-rw-r--r--  2.0 unx        5 b- defN 23-Jul-12 09:50 xpag-0.2.0.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     3184 b- defN 23-Jul-12 09:50 xpag-0.2.0.dist-info/RECORD
+40 files, 183267 bytes uncompressed, 50004 bytes compressed:  72.7%
```

## zipnote {}

```diff
@@ -99,23 +99,23 @@
 
 Filename: xpag/wrappers/gym_vec_env.py
 Comment: 
 
 Filename: xpag/wrappers/reset_done.py
 Comment: 
 
-Filename: xpag-0.1.9.dist-info/LICENSE
+Filename: xpag-0.2.0.dist-info/LICENSE
 Comment: 
 
-Filename: xpag-0.1.9.dist-info/METADATA
+Filename: xpag-0.2.0.dist-info/METADATA
 Comment: 
 
-Filename: xpag-0.1.9.dist-info/WHEEL
+Filename: xpag-0.2.0.dist-info/WHEEL
 Comment: 
 
-Filename: xpag-0.1.9.dist-info/top_level.txt
+Filename: xpag-0.2.0.dist-info/top_level.txt
 Comment: 
 
-Filename: xpag-0.1.9.dist-info/RECORD
+Filename: xpag-0.2.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## xpag/__init__.py

```diff
@@ -2,8 +2,8 @@
 from xpag import plotting
 from xpag import wrappers
 from xpag import buffers
 from xpag import samplers
 from xpag import agents
 from xpag import setters
 
-__version__ = "0.1.9"  # -version-
+__version__ = "0.2.0"  # -version-
```

## xpag/agents/sac/sac.py

```diff
@@ -7,14 +7,15 @@
 from xpag.agents.sac.sac_from_jaxrl import Batch, SACLearner
 from xpag.tools.utils import squeeze
 import functools
 from typing import Callable, Any, Tuple
 import flax
 import jax
 import jax.numpy as jnp
+import numpy as np
 
 
 @functools.partial(jax.jit, static_argnames="critic_apply_fn")
 def _qvalue(
     critic_apply_fn: Callable[..., Any],
     critic_params: flax.core.FrozenDict[str, Any],
     observations: jnp.ndarray,
@@ -70,20 +71,20 @@
     """
 
     def __init__(self, observation_dim, action_dim, params=None):
 
         self._config_string = str(list(locals().items())[1:])
         super().__init__("SAC", observation_dim, action_dim, params)
 
-        start_seed = 0 if "seed" not in params else params["seed"]
+        start_seed = np.random.randint(1e9) if "seed" not in params else params["seed"]
 
         self.saclearner_params = {
-            "actor_lr": 3e-3,
+            "actor_lr": 3e-4,
             "critic_lr": 3e-3,
-            "temp_lr": 3e-3,
+            "temp_lr": 3e-4,
             "backup_entropy": True,
             "discount": 0.99,
             "hidden_dims": (256, 256),
             "init_temperature": 1.0,
             "init_mean": None,
             "target_entropy": None,
             "target_update_period": 1,
```

## xpag/agents/sac/sac_from_jaxrl.py

```diff
@@ -47,14 +47,15 @@
 import optax
 import collections
 import functools
 from typing import Union, Any, Callable, Dict, Optional, Sequence, Tuple
 import flax.linen as nn
 import jax
 import jax.numpy as jnp
+from tensorflow_probability.substrates import jax as tfp
 
 
 def default_init(scale: Optional[float] = jnp.sqrt(2)):
     return nn.initializers.orthogonal(scale)
 
 
 PRNGKey = Any
@@ -411,16 +412,14 @@
         init_mean: Optional[jnp.ndarray],
         policy_final_fc_init_scale: float,
     ):
         """
         An implementation of the version of Soft-Actor-Critic described in
         https://arxiv.org/abs/1812.05905
         """
-        from tensorflow_probability.substrates import jax as tfp  # lazy import
-
         tfd = tfp.distributions
         tfb = tfp.bijectors
 
         class NormalTanhPolicy(nn.Module):
             hidden_dims: Sequence[int]
             action_dim: int
             state_dependent_std: bool = True
```

## xpag/agents/sdqn/sdqn.py

```diff
@@ -7,14 +7,15 @@
 import flax
 from flax import linen
 import jax
 import jax.numpy as jnp
 import optax
 from xpag.agents.agent import Agent
 from xpag.setters.setter import Setter
+import numpy as np
 
 Params = Any
 PRNGKey = jnp.ndarray
 
 
 @dataclasses.dataclass
 class FeedForwardModel:
@@ -45,21 +46,28 @@
     ):
         """
         Jax implementation of SDQN (https://arxiv.org/pdf/1705.05035.pdf).
         """
 
         discount = 0.99 if "discount" not in params else params["discount"]
         reward_scale = 1.0 if "reward_scale" not in params else params["reward_scale"]
-        actor_lr = 1e-3 if "actor_lr" not in params else params["actor_lr"]
-        critic_lr = 1e-3 if "critic_lr" not in params else params["critic_lr"]
+        critic_lr = 3e-3 if "critic_lr" not in params else params["critic_lr"]
+        critic_up_lr = (
+            critic_lr if "critic_up_lr" not in params else params["critic_up_lr"]
+        )
+        critic_low_lr = (
+            0.1 * critic_lr
+            if "critic_low_lr" not in params
+            else params["critic_low_lr"]
+        )
         soft_target_tau = 5e-2 if "tau" not in params else params["tau"]
         hidden_dims = (
             (256, 256) if "hidden_dims" not in params else params["hidden_dims"]
         )
-        start_seed = 0 if "seed" not in params else params["seed"]
+        start_seed = np.random.randint(1e9) if "seed" not in params else params["seed"]
         action_bins = 5 if "action_bins" not in params else params["action_bins"]
         # By default, actions are assumed to be between -1 and 1 across all
         # dimensions
         max_action = 1.0
         action_array = (
             jnp.tile(
                 jnp.arange(
@@ -215,28 +223,28 @@
         )
 
         key_models, key_q = jax.random.split(key_models)
         self.critic_up, self.critic_low = make_sdqn_networks(
             observation_dim, action_dim, self.action_bins, hidden_dims
         )
         self.critic_up_params = self.critic_up.init(key_q)
-        self.critic_up_optimizer = optax.adam(learning_rate=1.0 * critic_lr)
+        self.critic_up_optimizer = optax.adam(learning_rate=critic_up_lr)
         self.critic_up_optimizer_state = self.critic_up_optimizer.init(
             self.critic_up_params
         )
         list_critic_low_params = []
         for i, qmod in enumerate(self.critic_low):
             key_models, key_q = jax.random.split(key_models)
             qparams = qmod.init(key_q, i)
             list_critic_low_params.append(qparams)
 
         self.critic_low_params = flax.core.frozen_dict.FrozenDict(
             {str(i): list_critic_low_params[i] for i in range(len(self.critic_low))}
         )
-        self.critic_low_optimizer = optax.adam(learning_rate=1.0 * critic_lr)
+        self.critic_low_optimizer = optax.adam(learning_rate=critic_low_lr)
         self.critic_low_optimizer_state = self.critic_low_optimizer.init(
             self.critic_low_params
         )
 
         self.training_state = TrainingState(
             critic_up_optimizer_state=self.critic_up_optimizer_state,
             critic_up_params=self.critic_up_params,
```

## xpag/agents/td3/td3.py

```diff
@@ -8,14 +8,15 @@
 from flax import linen
 import jax
 import jax.numpy as jnp
 import optax
 from xpag.agents.agent import Agent
 import os
 import joblib
+import numpy as np
 
 Params = Any
 PRNGKey = jnp.ndarray
 
 
 @dataclasses.dataclass
 class FeedForwardModel:
@@ -41,28 +42,29 @@
     def __init__(
         self,
         observation_dim,
         action_dim,
         params=None,
     ):
         """
-        Jax implementation of TD3 (https://arxiv.org/abs/1802.09477).
+        Jax implementation of TD3 (https://arxiv.org/abs/1802.09477),
+        without delayed policy updates.
         This version assumes that the actions are between -1 and 1 (for all
         dimensions).
         """
 
         discount = 0.99 if "discount" not in params else params["discount"]
         reward_scale = 1.0 if "reward_scale" not in params else params["reward_scale"]
-        actor_lr = 3e-3 if "actor_lr" not in params else params["actor_lr"]
+        actor_lr = 3e-4 if "actor_lr" not in params else params["actor_lr"]
         critic_lr = 3e-3 if "critic_lr" not in params else params["critic_lr"]
         soft_target_tau = 5e-2 if "tau" not in params else params["tau"]
         hidden_dims = (
             (256, 256) if "hidden_dims" not in params else params["hidden_dims"]
         )
-        start_seed = 0 if "seed" not in params else params["seed"]
+        start_seed = np.random.randint(1e9) if "seed" not in params else params["seed"]
         # cpu, gpu or tpu backend
         self.backend = None if "backend" not in params else params["backend"]
 
         class CustomMLP(linen.Module):
             """MLP module."""
 
             layer_sizes: Sequence[int]
@@ -250,26 +252,14 @@
 
         def update_step(
             state: TrainingState, observations, actions, rewards, new_observations, mask
         ) -> Tuple[TrainingState, dict]:
 
             key, key_critic = jax.random.split(state.key, 2)
 
-            actor_l, actor_grads = self.actor_grad(
-                state.policy_params, state.target_q_params, observations
-            )
-
-            policy_params_update, policy_optimizer_state = self.policy_optimizer.update(
-                actor_grads, state.policy_optimizer_state
-            )
-
-            policy_params = optax.apply_updates(
-                state.policy_params, policy_params_update
-            )
-
             critic_l, critic_grads = self.critic_grad(
                 state.q_params,
                 state.target_policy_params,
                 state.target_q_params,
                 observations,
                 actions,
                 new_observations,
@@ -285,14 +275,26 @@
 
             new_target_q_params = jax.tree_util.tree_map(
                 lambda x, y: x * (1 - soft_target_tau) + y * soft_target_tau,
                 state.target_q_params,
                 q_params,
             )
 
+            actor_l, actor_grads = self.actor_grad(
+                state.policy_params, q_params, observations
+            )
+
+            policy_params_update, policy_optimizer_state = self.policy_optimizer.update(
+                actor_grads, state.policy_optimizer_state
+            )
+
+            policy_params = optax.apply_updates(
+                state.policy_params, policy_params_update
+            )
+
             new_target_policy_params = jax.tree_util.tree_map(
                 lambda x, y: x * (1 - soft_target_tau) + y * soft_target_tau,
                 state.target_policy_params,
                 policy_params,
             )
 
             new_state = TrainingState(
```

## xpag/agents/tqc/tqc.py

```diff
@@ -47,14 +47,15 @@
 import functools
 from typing import Callable, Any, Tuple, Sequence, Optional
 import flax
 import flax.linen as nn
 import jax
 import optax
 import jax.numpy as jnp
+import numpy as np
 
 
 @functools.partial(jax.jit, static_argnames="critic_apply_fn")
 def _qvalue(
     critic_apply_fn: Callable[..., Any],
     critic_params: flax.core.FrozenDict[str, Any],
     observations: jnp.ndarray,
@@ -337,22 +338,22 @@
 class TQC(Agent):
     def __init__(self, observation_dim, action_dim, params=None):
         """
         Interface to TQC agent
         """
 
         self._config_string = str(list(locals().items())[1:])
-        super().__init__("SAC", observation_dim, action_dim, params)
+        super().__init__("TQC", observation_dim, action_dim, params)
 
-        start_seed = 0 if "seed" not in params else params["seed"]
+        start_seed = np.random.randint(1e9) if "seed" not in params else params["seed"]
 
         self.tqclearner_params = {
-            "actor_lr": 3e-3,
+            "actor_lr": 3e-4,
             "critic_lr": 3e-3,
-            "temp_lr": 3e-3,
+            "temp_lr": 3e-4,
             "backup_entropy": True,
             "discount": 0.99,
             "hidden_dims_actor": (256, 256),
             "hidden_dims_critic": (256, 256),
             "init_temperature": 1.0,
             "init_mean": None,
             "target_entropy": None,
```

## xpag/tools/learn.py

```diff
@@ -132,15 +132,15 @@
                 else hstack(observation["observation"], observation["desired_goal"]),
                 eval_mode=False,
             )
             if isinstance(action, tuple):
                 action_info = action[1]
                 action = action[0]
             if i > 0:
-                for _ in range(max(gd_steps_per_step * env_info["num_envs"], 1)):
+                for _ in range(gd_steps_per_step * env_info["num_envs"]):
                     _ = agent.train_on_batch(buffer.sample(batch_size))
 
         action = datatype_convert(action, env_datatype)
 
         (
             observation,
             action,
```

## xpag/wrappers/brax_vec_env.py

```diff
@@ -8,15 +8,15 @@
 import jax.numpy as jnp
 import gymnasium as gym
 import numpy as np
 from gymnasium import spaces
 from gymnasium.vector import utils
 from xpag.tools.utils import get_env_dimensions
 from brax import envs
-from brax.envs import env as brax_env
+from brax.envs import Wrapper, State
 
 _envs_episode_length = {
     "acrobot": 1000,
     "ant": 1000,
     "fast": 1000,
     "fetch": 1000,
     "grasp": 1000,
@@ -37,27 +37,25 @@
 def brax_vec_env_(
     env_name: str,
     num_envs: int,
     wrap_function: Callable = None,
     *,
     force_cpu_backend=False,
 ):
-    class ResetDoneBraxWrapper(brax_env.Wrapper):
+    class ResetDoneBraxWrapper(Wrapper):
         """Adds reset_done() to Brax envs."""
 
-        def reset(self, rng: jnp.ndarray) -> brax_env.State:
+        def reset(self, rng: jnp.ndarray) -> State:
             state = self.env.reset(rng)
             return state
 
-        def step(self, state: brax_env.State, action: jnp.ndarray) -> brax_env.State:
+        def step(self, state: State, action: jnp.ndarray) -> State:
             return self.env.step(state, action)
 
-        def reset_done(
-            self, done: jnp.ndarray, state: brax_env.State, rng: jnp.ndarray
-        ):
+        def reset_done(self, done: jnp.ndarray, state: State, rng: jnp.ndarray):
             # done = state.done
             def where_done(x, y):
                 done_ = done
                 if done_.shape:
                     done_ = jnp.reshape(
                         done_, tuple([x.shape[0]] + [1] * (len(x.shape) - 1))
                     )  # type: ignore
```

## Comparing `xpag-0.1.9.dist-info/LICENSE` & `xpag-0.2.0.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `xpag-0.1.9.dist-info/METADATA` & `xpag-0.2.0.dist-info/METADATA`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: xpag
-Version: 0.1.9
+Version: 0.2.0
 Summary: xpag: Exploring Agents
 Home-page: https://github.com/perrin-isir/xpag
 Author: Nicolas Perrin-Gilbert
 License: LICENSE
 Description-Content-Type: text/markdown
 License-File: LICENSE
 Requires-Dist: psutil (>=5.8.0)
@@ -13,21 +13,21 @@
 Requires-Dist: joblib (>=1.1.0)
 Requires-Dist: gymnasium (>=0.28.1)
 Requires-Dist: Pillow (>=9.0.1)
 Requires-Dist: ipywidgets (>=7.6.5)
 Requires-Dist: jax (>=0.4.8)
 Requires-Dist: optax (>=0.1.2)
 Requires-Dist: flax (>=0.6.3)
-Requires-Dist: brax (>=0.9.0)
+Requires-Dist: brax (>=0.9.1)
 Requires-Dist: tensorflow-probability (>=0.15.0)
 Requires-Dist: mediapy (>=1.1.4)
 
 # ![alt text](https://raw.githubusercontent.com/perrin-isir/xpag/main/logo.png "xpag logo")
 
-![version](https://img.shields.io/badge/version-0.1.9-blue)
+![version](https://img.shields.io/badge/version-0.2.0-blue)
 [![codestyle](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
 [![Documentation](https://img.shields.io/github/actions/workflow/status/perrin-isir/xpag/docs.yml?branch=main&label=docs)](https://perrin-isir.github.io/xpag/)
 [![PyPI version](https://img.shields.io/pypi/v/xpag)](https://pypi.org/project/xpag/)
 
 
 *xpag* ("e**xp**loring **ag**ents") is a modular reinforcement learning library with JAX agents, currently in beta version.
```

## Comparing `xpag-0.1.9.dist-info/RECORD` & `xpag-0.2.0.dist-info/RECORD`

 * *Files 8% similar despite different names*

```diff
@@ -1,40 +1,40 @@
-xpag/__init__.py,sha256=EYp8A3CVBVkt6zmJiyWhpFgyqCtIP_-0_D7Mv83KNOM,211
+xpag/__init__.py,sha256=Bepl6faeJ2YZ68asmPzfM908n3tkrQDwrfjEg3g8sKM,211
 xpag/agents/__init__.py,sha256=qwWcvnIIALsfDemnUNkyQqZNAbRoNbBkS9ppu0BjjRc,204
 xpag/agents/agent.py,sha256=sMvWydFTU1f4kar6nnSbvXSi56tlEla6NJiwgVnM47s,1098
 xpag/agents/sac/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-xpag/agents/sac/sac.py,sha256=xLKisxVIAmQ5FA4E4ppV3O7W8HyWgUu0VkVZzGJvsik,5554
-xpag/agents/sac/sac_from_jaxrl.py,sha256=2Nbr3jg96Ohbwcjom10WH0Pcu5ipH058KI66_LFZYUk,18817
+xpag/agents/sac/sac.py,sha256=m7-zsxqdmKAVlt-4Op698qEGXhGy1AqE1zDT1F7iJ5k,5594
+xpag/agents/sac/sac_from_jaxrl.py,sha256=oawVjB47QaYImpMeCx3YJ1E160u-kkfYN8d8E29MVVk,18793
 xpag/agents/sdqn/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-xpag/agents/sdqn/sdqn.py,sha256=0CP9Se8S9QIpMUjHJmFAQBulGzAd6FQHKbkQWhCtLSY,22063
+xpag/agents/sdqn/sdqn.py,sha256=-RWzLJA5G_TyBiyUOA60QffstJ6hRgirw6nHOT4Ssaw,22289
 xpag/agents/td3/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-xpag/agents/td3/td3.py,sha256=gUeUWn9NAxAwYCbzhN4Yax6d6S5R4D_MbcFrA_js_1w,15221
+xpag/agents/td3/td3.py,sha256=Zmo_kD2dnqrO-3n0iphCzgFAaJJViWUQnqqc0la6FX0,15288
 xpag/agents/tqc/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-xpag/agents/tqc/tqc.py,sha256=Ui_nW4kLvWf1nvyaijCC7ejdH3MMn2Ij1MeI-JbVclY,12988
+xpag/agents/tqc/tqc.py,sha256=IpDoOuVXObSFPnfHZioBEoNYXjvUSa1feO46fCQHskQ,13028
 xpag/buffers/__init__.py,sha256=iB2C6BNDBf5yFtI_27iuCIUXjL_pgJcMlC1ZQRN_ATw,160
 xpag/buffers/buffer.py,sha256=UjNieWaV0mTEaol7HjGDhWr3ll8Vcwy72Kl3M3OwLqE,11221
 xpag/buffers/jax_buffer.py,sha256=mKlFwkGt4b5NcJbY9RB_W04aqk3kDs3U8M63oooWGnM,5236
 xpag/plotting/__init__.py,sha256=Yt8egGFZIAgrAlcKFzfHExb7ky28BSD3E-igrBYFxTU,55
 xpag/plotting/plotting.py,sha256=h3JH9QpmOgDhIXXvqGF31FqrIF-nud731IlAkPgIPJ0,4086
 xpag/samplers/HER.py,sha256=xmjKZv4eCOPudddqDnNW-Vqo5TEol_SToRSqzQ74P0k,2841
 xpag/samplers/__init__.py,sha256=fazYtUuLPWpbDxZxB40VRUZcYuE7P-LnRpFuoTHMSkE,182
 xpag/samplers/jax_sampler.py,sha256=iPLVriHQu_X-YbSxbYMTA1gV7TKYhS186rTpEbR2c40,2941
 xpag/samplers/sampler.py,sha256=9At5VBYL5ziH7UzCkA8du_zSjPjLTx0sI81AVhSOoBw,1917
 xpag/setters/__init__.py,sha256=gEEbf529oUwVHTpStUelg0Y7NQ8qQxbCK_Mnhv0pb6I,71
 xpag/setters/setter.py,sha256=Rey6VOm8T8gEhVql_2LhQIN6ZKC0lC5J6aAOizPq5Rs,3734
 xpag/tools/__init__.py,sha256=6PpnqZEtELXPQ_vWnyp-l6ZnIVFXFiaMWhaTZ7XEJoI,438
 xpag/tools/eval.py,sha256=8ceSq-WiZTljQE1IE220iR1b1WodqgHsFfaxu88CK0k,5065
-xpag/tools/learn.py,sha256=Ow63UUpeW5g6vx1Hzz_wVggrCwQOfa24eyc18-_ct60,7611
+xpag/tools/learn.py,sha256=Yoa1hnuINfPySGDt7BzgpZW-_W9v8bljpdRZHkBcTh0,7603
 xpag/tools/logging.py,sha256=Ip3ETtLHYyqlmC8Z2BncuaI_y2RNBN99-UF1VoEWeuE,3225
 xpag/tools/replay.py,sha256=ixLRRzDgcg9NtQtbxPKTj69tpGsU4Xtop8QiL__TpfA,4708
 xpag/tools/timing.py,sha256=aihoZOw4xCML2hrgpqWs_TgFIOThPqJuyFG7weod9-Y,899
 xpag/tools/utils.py,sha256=4ptg9fqSaMWWVAVXcENarK7gBfvJuIDoNW_lm1mHrsM,6933
 xpag/wrappers/__init__.py,sha256=4tECDefZtReXVN3tUyIbfNeO-Q-hXA9Pm0e5vOMKwgA,179
-xpag/wrappers/brax_vec_env.py,sha256=NvB7BvuPhWz9OSNtyz7W6infeViVRM5OOauA43OAJiE,8257
+xpag/wrappers/brax_vec_env.py,sha256=NsHU-Zt2aPxfXiVtrSTG_0cF5JZPGEsiuQxQ9XghvIs,8189
 xpag/wrappers/goalenv_wrapper.py,sha256=G4wmfUn7d6ae-O3b8273ZaPP-bmj6Bx8nf7QDiN3mls,6099
 xpag/wrappers/gym_vec_env.py,sha256=VQ2Q5xUa8dgpLeshqIoLNGxKzJ3CrvyG5__lFlaaZzk,10148
 xpag/wrappers/reset_done.py,sha256=GJHmSIpJuQNkiO-rKg0oDG0np7_CbZkBPJ-4akSpypY,936
-xpag-0.1.9.dist-info/LICENSE,sha256=g85jiSLf3DU-cU426LLpqKUqjFwcnfYF8qpyms8R9p8,1517
-xpag-0.1.9.dist-info/METADATA,sha256=k_gYZB0frPqLZth7lpNtHjbguCuTZ57j0JLAVzwkKAQ,15098
-xpag-0.1.9.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
-xpag-0.1.9.dist-info/top_level.txt,sha256=CUClY4Z7dB5zr_iqoqBK479ZNT9uDDCD7hWIHgQTDpc,5
-xpag-0.1.9.dist-info/RECORD,,
+xpag-0.2.0.dist-info/LICENSE,sha256=g85jiSLf3DU-cU426LLpqKUqjFwcnfYF8qpyms8R9p8,1517
+xpag-0.2.0.dist-info/METADATA,sha256=StC8u2KvIH42lTdvP2h9dMszAp_VhEcvNjBkyevKpgc,15098
+xpag-0.2.0.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
+xpag-0.2.0.dist-info/top_level.txt,sha256=CUClY4Z7dB5zr_iqoqBK479ZNT9uDDCD7hWIHgQTDpc,5
+xpag-0.2.0.dist-info/RECORD,,
```

