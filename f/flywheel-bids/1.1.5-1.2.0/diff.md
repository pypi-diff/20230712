# Comparing `tmp/flywheel_bids-1.1.5-py3-none-any.whl.zip` & `tmp/flywheel_bids-1.2.0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,22 +1,22 @@
-Zip file size: 103720 bytes, number of entries: 54
+Zip file size: 105639 bytes, number of entries: 54
 -rw-r--r--  2.0 unx        5 b- defN 80-Jan-01 00:00 flywheel_bids/__init__.py
--rw-r--r--  2.0 unx    22103 b- defN 80-Jan-01 00:00 flywheel_bids/curate_bids.py
--rw-r--r--  2.0 unx    21920 b- defN 80-Jan-01 00:00 flywheel_bids/export_bids.py
+-rw-r--r--  2.0 unx    22562 b- defN 80-Jan-01 00:00 flywheel_bids/curate_bids.py
+-rw-r--r--  2.0 unx    23671 b- defN 80-Jan-01 00:00 flywheel_bids/export_bids.py
 -rw-r--r--  2.0 unx       76 b- defN 80-Jan-01 00:00 flywheel_bids/results/__init__.py
 -rw-r--r--  2.0 unx     2361 b- defN 80-Jan-01 00:00 flywheel_bids/results/zip_htmls.py
 -rw-r--r--  2.0 unx     5147 b- defN 80-Jan-01 00:00 flywheel_bids/results/zip_intermediate.py
 -rw-r--r--  2.0 unx        5 b- defN 80-Jan-01 00:00 flywheel_bids/supporting_files/__init__.py
--rw-r--r--  2.0 unx    11161 b- defN 80-Jan-01 00:00 flywheel_bids/supporting_files/bidsify_flywheel.py
+-rw-r--r--  2.0 unx    10537 b- defN 80-Jan-01 00:00 flywheel_bids/supporting_files/bidsify_flywheel.py
 -rw-r--r--  2.0 unx     4353 b- defN 80-Jan-01 00:00 flywheel_bids/supporting_files/classifications.py
 -rw-r--r--  2.0 unx     1234 b- defN 80-Jan-01 00:00 flywheel_bids/supporting_files/errors.py
 -rw-r--r--  2.0 unx     6607 b- defN 80-Jan-01 00:00 flywheel_bids/supporting_files/file_funcs.py
 -rw-r--r--  2.0 unx     7311 b- defN 80-Jan-01 00:00 flywheel_bids/supporting_files/project_tree.py
--rw-r--r--  2.0 unx     4208 b- defN 80-Jan-01 00:00 flywheel_bids/supporting_files/resolver.py
--rw-r--r--  2.0 unx    23580 b- defN 80-Jan-01 00:00 flywheel_bids/supporting_files/templates.py
+-rw-r--r--  2.0 unx     5566 b- defN 80-Jan-01 00:00 flywheel_bids/supporting_files/resolver.py
+-rw-r--r--  2.0 unx    23538 b- defN 80-Jan-01 00:00 flywheel_bids/supporting_files/templates.py
 -rw-r--r--  2.0 unx    10872 b- defN 80-Jan-01 00:00 flywheel_bids/supporting_files/utils.py
 -rw-r--r--  2.0 unx      175 b- defN 80-Jan-01 00:00 flywheel_bids/templates/README.md
 -rw-r--r--  2.0 unx    27899 b- defN 80-Jan-01 00:00 flywheel_bids/templates/bids-v1.json
 -rw-r--r--  2.0 unx    53901 b- defN 80-Jan-01 00:00 flywheel_bids/templates/default.json
 -rw-r--r--  2.0 unx     1536 b- defN 80-Jan-01 00:00 flywheel_bids/templates/flywheel_curated/README.md
 -rw-r--r--  2.0 unx      271 b- defN 80-Jan-01 00:00 flywheel_bids/templates/flywheel_curated/bridge-bdlong-project-template-attributes.txt
 -rw-r--r--  2.0 unx    38666 b- defN 80-Jan-01 00:00 flywheel_bids/templates/flywheel_curated/bridge-bdlong-project-template.json
@@ -38,19 +38,19 @@
 -rw-r--r--  2.0 unx     6031 b- defN 80-Jan-01 00:00 flywheel_bids/templates/flywheel_curated/columbia-tottenham-pacct-project-template.json
 -rw-r--r--  2.0 unx      264 b- defN 80-Jan-01 00:00 flywheel_bids/templates/flywheel_curated/nyu-bair-vfs-project-template-attributes.txt
 -rw-r--r--  2.0 unx     8483 b- defN 80-Jan-01 00:00 flywheel_bids/templates/flywheel_curated/nyu-bair-vfs-project-template.json
 -rw-r--r--  2.0 unx      276 b- defN 80-Jan-01 00:00 flywheel_bids/templates/flywheel_curated/reproin-philips-project-template-attributes.txt
 -rw-r--r--  2.0 unx    30276 b- defN 80-Jan-01 00:00 flywheel_bids/templates/flywheel_curated/reproin-philips-project-template.json
 -rw-r--r--  2.0 unx      269 b- defN 80-Jan-01 00:00 flywheel_bids/templates/flywheel_curated/reproin-reproin-project-template-attributes.txt
 -rw-r--r--  2.0 unx    42094 b- defN 80-Jan-01 00:00 flywheel_bids/templates/flywheel_curated/reproin-reproin-project-template.json
--rw-r--r--  2.0 unx    44581 b- defN 80-Jan-01 00:00 flywheel_bids/templates/reproin.json
--rw-r--r--  2.0 unx    57906 b- defN 80-Jan-01 00:00 flywheel_bids/upload_bids.py
+-rw-r--r--  2.0 unx    44997 b- defN 80-Jan-01 00:00 flywheel_bids/templates/reproin.json
+-rw-r--r--  2.0 unx    63150 b- defN 80-Jan-01 00:00 flywheel_bids/upload_bids.py
 -rw-r--r--  2.0 unx    13998 b- defN 80-Jan-01 00:00 flywheel_bids/utils/download_run_level.py
 -rw-r--r--  2.0 unx     2026 b- defN 80-Jan-01 00:00 flywheel_bids/utils/performance.py
 -rw-r--r--  2.0 unx     1816 b- defN 80-Jan-01 00:00 flywheel_bids/utils/run_level.py
 -rw-r--r--  2.0 unx     2522 b- defN 80-Jan-01 00:00 flywheel_bids/utils/tree.py
 -rw-r--r--  2.0 unx     5898 b- defN 80-Jan-01 00:00 flywheel_bids/utils/validate.py
--rw-r--r--  2.0 unx     5574 b- defN 80-Jan-01 00:00 flywheel_bids-1.1.5.dist-info/METADATA
--rw-r--r--  2.0 unx       88 b- defN 80-Jan-01 00:00 flywheel_bids-1.1.5.dist-info/WHEEL
--rw-r--r--  2.0 unx     1066 b- defN 80-Jan-01 00:00 flywheel_bids-1.1.5.dist-info/LICENSE
-?rw-r--r--  2.0 unx     6263 b- defN 16-Jan-01 00:00 flywheel_bids-1.1.5.dist-info/RECORD
-54 files, 538142 bytes uncompressed, 93098 bytes compressed:  82.7%
+-rw-r--r--  2.0 unx     5627 b- defN 80-Jan-01 00:00 flywheel_bids-1.2.0.dist-info/METADATA
+-rw-r--r--  2.0 unx       88 b- defN 80-Jan-01 00:00 flywheel_bids-1.2.0.dist-info/WHEEL
+-rw-r--r--  2.0 unx     1066 b- defN 80-Jan-01 00:00 flywheel_bids-1.2.0.dist-info/LICENSE
+?rw-r--r--  2.0 unx     6263 b- defN 16-Jan-01 00:00 flywheel_bids-1.2.0.dist-info/RECORD
+54 files, 546757 bytes uncompressed, 95017 bytes compressed:  82.6%
```

## zipnote {}

```diff
@@ -144,20 +144,20 @@
 
 Filename: flywheel_bids/utils/tree.py
 Comment: 
 
 Filename: flywheel_bids/utils/validate.py
 Comment: 
 
-Filename: flywheel_bids-1.1.5.dist-info/METADATA
+Filename: flywheel_bids-1.2.0.dist-info/METADATA
 Comment: 
 
-Filename: flywheel_bids-1.1.5.dist-info/WHEEL
+Filename: flywheel_bids-1.2.0.dist-info/WHEEL
 Comment: 
 
-Filename: flywheel_bids-1.1.5.dist-info/LICENSE
+Filename: flywheel_bids-1.2.0.dist-info/LICENSE
 Comment: 
 
-Filename: flywheel_bids-1.1.5.dist-info/RECORD
+Filename: flywheel_bids-1.2.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## flywheel_bids/curate_bids.py

```diff
@@ -6,15 +6,16 @@
 import re
 import sys
 import tempfile
 from pathlib import Path
 
 import flywheel
 
-from .supporting_files import bidsify_flywheel, templates, utils
+from .supporting_files import bidsify_flywheel, utils
+from .supporting_files.templates import load_template
 from .supporting_files.project_tree import get_project_node, set_tree
 
 logger = logging.getLogger("curate-bids")
 
 
 def clear_meta_info(context, template):
     if "info" in context:
@@ -28,15 +29,14 @@
     path = "/".join(err.path)
     if path:
         return path + " " + err.message
     return err.message
 
 
 def print_error_log(container, templateDef, errors):
-
     BIDS = container["info"].get("BIDS")
     bids_template = BIDS.get("template")
 
     error_message = ""
 
     if not bids_template:
         error_message += (
@@ -51,15 +51,14 @@
 
     for ie, ee in enumerate(errors):
         error_message += (
             f"{ie+1}:\n............................................................\n"
         )
 
         if ee.validator == "required":
-
             match = re.match("'(?P<prop>.*)' is a required property", ee.message)
             required_prop = match.group("prop")
             schema = templateDef.get("properties", {}).get(required_prop)
             schema.pop("auto_update", "")
 
             if schema is None:
                 error_message += (
@@ -124,15 +123,14 @@
         # Find template
         templateName = container["info"][namespace].get("template")
         if templateName:
             templateDef = template.definitions.get(templateName)
             if templateDef:
                 errors = template.validate(templateDef, container["info"][namespace])
                 if errors:
-
                     log_error = print_error_log(container, templateDef, errors)
                     logger.debug(log_error)
 
                     valid = False
                     error_message = "\n".join(
                         [format_validation_error(err) for err in errors]
                     )
@@ -187,99 +185,93 @@
             context["acquisition"]["id"], context["acquisition"]["info"]
         )
     # Cannot determine container type
     else:
         logger.info("Cannot determine container type: " + context["container_type"])
 
 
-def set_up_template(fw, project, template_type=None, template_file=None):
-
-    template = None
-
-    # If a template file is provided, this supersedes template type
-    if template_file:
-        template_type = None
-        template = templates.loadTemplate(template_file)
-        logger.info("Using project curation template: '%s'", template_file)
-
-    # If neither a template file nor a template type are indicated, the the Default template is selected
-    if not template_file and not template_type:
-        template_type = "ReproIn"
-
-    if template_type:
-
-        if template_type == "BIDS-v1":
-            logger.info(
-                "Using project curation template: Flywheel BIDS-v1: "
-                "https://gitlab.com/flywheel-io/public/bids-client/-/blob/master/flywheel_bids/templates/bids-v1.json"
-            )
-            template = templates.BIDS_V1_TEMPLATE
-
-        elif template_type == "ReproIn":
-            logger.info(
-                "Using project curation template: Flywheel ReproIn: "
-                "https://gitlab.com/flywheel-io/public/bids-client/-/blob/master/flywheel_bids/templates/reproin.json"
-            )
-            template = templates.REPROIN_TEMPLATE
-
-        else:
-            logger.info(
-                "Project curation base template must be 'BIDS-v1' or 'ReproIn', not %s. Exiting.",
-                template_type,
-            )
-            # TODO: Create custom error for exit
-            os.sys.exit(1)
-
-    return template
-
-
 class Count:
     def __init__(self):
         self.containers = 0  # counts project, sessions and acquisitions
         self.sessions = 0
         self.acquisitions = 0
         self.files = 0
 
 
+def save_project_sidecar(project, project_node):
+    """Save "dataset_description.json" sidecar file
+
+    Remove info that was just put on the project_node and attach it as a file
+    on the project if it does not yet exist.
+
+    Args:
+        project (Project container)
+        project_node (TreeNode) a "context" representation of the project
+    """
+
+    SIDECAR_NAME = "dataset_description.json"
+    proj_sidecars = [file for file in project.files if file.name == SIDECAR_NAME]
+
+    if len(proj_sidecars) > 1:
+        logger.debug("ERROR: multiple '%s' files exist", SIDECAR_NAME)
+    elif len(proj_sidecars) == 1:
+        logger.info("'%s' file already exists", SIDECAR_NAME)
+    else:  # create and attach file to project
+        logger.info("'%s' file does not exist, creating it...", SIDECAR_NAME)
+        json_data = project_node.data["info"]["BIDS"]
+        template_val = json_data.pop("template")
+        rule_id_val = json_data.pop("rule_id")
+        json_str = json.dumps(json_data, indent=4)
+        file_spec = flywheel.FileSpec(SIDECAR_NAME, json_str, "text/plain")
+        project.upload_file(file_spec)
+        project_node.data["info"]["BIDS"] = {
+            "template": template_val,
+            "rule_id": rule_id_val,
+        }
+
+
 def curate_bids(
     fw,
     project_id,
     subject_id="",
     session_id="",
     reset=False,
     dont_recurate_project=False,
-    template_type="",
-    template_file="",
+    template_name="",
+    template_path="",
     pickle_tree=False,
     dry_run=False,
+    save_sidecar_as_metadata=False,
 ):
     """Curate BIDS.
 
     If curating an entire project, loop over subjects to find all sessions.  Curate all sessions for a given
     subject at the same time so "resolvers" can work on all sessions for the subject.
     This can run on only one subject or session if desired by providing an ID.
 
     Args:
         fw (Flywheel Client): The Flywheel Client
         project_id (str): The Flywheel Project container ID.
         subject_id (str): The Flywheel subject container ID (will only curate this subject).
         session_id (str): The Flywheel session container ID (will only curate this session).
         reset (bool): Whether to erase info.BIDS before curation.
         dont_recurate_project (bool): If project container is already curated, make this True
-        template_type (str): Which template type to use. Options include:
+        template_name (str): Which template type to use. Options include:
                 Default, BIDS-v1, ReproIn.
-        template_file (str): Provide a specific template file. Supersedes template_type.
+        template_path (str): Provide a specific template file. Supersedes template_name.
+        save_sidecar_as_metadata (bool): sidecar data is in file.info (metadata) so for
+                IntendedFors, update metadata instead of updating the actual json sidecar.
 
     """
 
     count = Count()
 
     project = fw.get_project(project_id)
 
-    template = set_up_template(fw, project, template_type, template_file)
+    template = load_template(fw, template_path, template_name, save_sidecar_as_metadata)
 
     p_name = f"project_node_{project_id}.pickle"
     if pickle_tree and Path(p_name).exists():
         logger.info("Using pickled %s", p_name)
         with open(p_name, "rb") as f:
             project_node = pickle.load(f)
     else:
@@ -296,14 +288,17 @@
         project_node,
         count,
         reset=reset,
         dont_recurate_project=dont_recurate_project,
         dry_run=dry_run,
     )
 
+    if not save_sidecar_as_metadata:  # then (default) save project metadata as sidecar
+        save_project_sidecar(project, project_node)
+
     if session_id:
         logger.info("Getting single session ID=%s", session_id)
         session = fw.get_session(session_id)
         subject_id = session.subject.id
 
     if subject_id:
         logger.info("Getting single subject ID=%s", subject_id)
@@ -385,15 +380,14 @@
         count (Count)
     """
     # Curation begins: match, resolve, update
 
     # Match: do initial template matching and updating
 
     for context in project_node.context_iter():
-
         ctype = context["container_type"]
         parent_ctype = context["parent_container_type"]
 
         if ctype == "project" and dont_recurate_project:
             pass  # don't curate OR reset
         else:
             # Cleanup, if indicated
@@ -401,15 +395,14 @@
                 clear_meta_info(context[ctype], template)
 
             elif context[ctype].get("info", {}).get("BIDS") == "NA":
                 continue
 
         # BIDSIFY: note that subjects are not bidsified because they have no BIDS information on them.
         if ctype in ["project", "session", "acquisition"]:
-
             if ctype == "project":
                 if dont_recurate_project:
                     logger.debug("Not re-curating project container")
                     continue
 
             logger.info(
                 f"{count.containers}: Bidsifying Container: <{ctype}> <{context.get(ctype).get('label')}>"
@@ -427,36 +420,35 @@
             if ctype == "session":
                 logger.debug(
                     f"adding run counter for session {context.get(ctype).get('label')}"
                 )
                 context["run_counters"] = utils.RunCounterMap()
 
         elif ctype == "file":
-
             logger.debug(
                 f"Bidsifying file: <{ctype}> <{context.get(ctype).get('name')}>"
             )
 
             count.files += 1
 
             # Process matching
             context["file"] = bidsify_flywheel.process_matching_templates(
                 context, template
             )
 
             # Validate meta information
             validate_meta_info(context["file"], template)
 
-    # Resolve: perform path resolutions, if needed.  Currently only used to handle "IntendedFor" field which
-    # needs to happen after a subject has been curated.
-    for context in project_node.context_iter():
-        bidsify_flywheel.process_resolvers(context, template)
-
-    # Update: send updates to Flywheel, if the Flywheel Client is instantiated
     if not dry_run:
+        # Resolve: perform path resolutions, if needed.  Currently only used to handle "IntendedFor" field which
+        # needs to happen after a subject has been curated.
+        for context in project_node.context_iter():
+            bidsify_flywheel.process_resolvers(context, template)
+
+        # Update: send updates to Flywheel, if the Flywheel Client is instantiated
         if fw:
             logger.info("Updating BIDS metadata on Flywheel")
             for context in project_node.context_iter():
                 ctype = context["container_type"]
                 node = context[ctype]
                 if node.is_dirty():
                     update_meta_info(fw, context)
@@ -495,16 +487,16 @@
 
 
 def main_with_args(
     api_key,
     session_id,
     reset,
     session_only,
-    template_type,
-    template_file=None,
+    template_name,
+    template_path=None,
     subject_id="",
     project_label="",
     group_id="",
     verbosity=1,
     pickle_tree=False,
     dry_run=False,
 ):
@@ -535,23 +527,22 @@
     curate_bids(
         fw,
         project_id,
         subject_id=subject_id,
         session_id=session_id,
         reset=reset,
         dont_recurate_project=False,
-        template_type=template_type,
-        template_file=template_file,
+        template_name=template_name,
+        template_path=template_path,
         pickle_tree=pickle_tree,
         dry_run=dry_run,
     )
 
 
 def main():
-
     parser = argparse.ArgumentParser(description="BIDS Curation")
     parser.add_argument(
         "--api-key", dest="api_key", action="store", required=True, help="API key"
     )
     parser.add_argument(
         "-p",
         dest="project_label",
@@ -596,23 +587,23 @@
         dest="dont_recurate_project",
         action="store_true",
         default=False,
         help="Don't re-curate the project.",
     )
     parser.add_argument(
         "--template-type",
-        dest="template_type",
+        dest="template_name",
         action="store",
         required=False,
         default=None,
         help="Which template type to use. Options include : Default, ReproIn, or Custom.",
     )
     parser.add_argument(
         "--template-file",
-        dest="template_file",
+        dest="template_path",
         action="store",
         default=None,
         help="Template file to use. Supersedes the --template-type flag.",
     )
     parser.add_argument(
         "--pickle_tree",
         dest="pickle_tree",
@@ -631,14 +622,20 @@
         "--verbosity",
         dest="verbosity",
         action="store",
         type=int,
         default=1,
         help="Debug level (0, 10, 2, 20)",
     )
+    parser.add_argument(
+        "--save_sidecar_as_metadata",
+        default=False,
+        required=False,
+        help="The BIDS sidecar is metadata in file.info. (default = false)",
+    )
 
     args = parser.parse_args()
 
     configure_logging(int(args.verbosity))
 
     # Prep
     # Check API key - raises Error if key is invalid
@@ -664,16 +661,17 @@
     curate_bids(
         fw,
         project_id,
         args.subject_id,
         args.session_id,
         reset=args.reset,
         dont_recurate_project=False,
-        template_type=args.template_type,
-        template_file=args.template_file,
+        template_name=args.template_name,
+        template_path=args.template_path,
         pickle_tree=args.pickle_tree,
         dry_run=args.dry_run,
+        save_sidecar_as_metadata=args.save_sidecar_as_metadata,
     )
 
 
 if __name__ == "__main__":
     main()
```

## flywheel_bids/export_bids.py

```diff
@@ -4,15 +4,17 @@
 import os
 import re
 import sys
 import zipfile
 
 import dateutil.parser
 import flywheel
+from datetime import datetime
 
+from pathlib import Path
 from flywheel_bids.supporting_files import errors, utils
 from flywheel_bids.supporting_files.errors import BIDSExportError
 
 logging.basicConfig(level=logging.INFO)
 logger = logging.getLogger("bids-exporter")
 
 EPOCH = dateutil.parser.parse("1970-01-01 00:00:0Z")
@@ -150,111 +152,145 @@
     metadata = get_metadata(f, namespace)
     if not metadata:
         return ""
 
     return metadata.get("Folder")
 
 
-def create_json(meta_info, path, namespace):
+def check_sidecar_exist(fw, filepath_downloads, outdir):
+    """Match acquisitions to be downloaded to sidecar.
+    If the sidecar exists, then nothing is changed or created.
+    If the sidecar does not exist, then the sidecar is created from the metadata
+    and the newly generated sidecar is added to list of files to download.
+    """
+
+    for acq, acq_details in filepath_downloads["acquisition"].items():
+        if not Path(acq).stem in filepath_downloads["sidecar"]:
+            # Need to get acquisition for the metadata
+            f = [
+                x
+                for x in fw.get_acquisition(acq_details["args"][0]).get("files", {})
+                if acq_details["args"][1] == x["name"]
+            ][0]
+            try:
+                create_json_sidecar(f["info"], acq_details["args"][1], "BIDS", outdir)
+            except:
+                logger.error(f"{f['name']} does not have info section.")
+
+
+def create_json_sidecar(metadata, path, namespace, outdir):
     """
     Create a JSON file with the BIDS info.
 
     Given a dictionary of the meta info and the path, creates a JSON file
-        with the bids info.
+    with the FW metadata, except for BIDS info.
 
     namespace in the template namespace, in this case it is 'BIDS'.
 
     Args:
-        meta_info (dict) : dict with the metadata
+        info (str or dict) : str is the acq_id; dict is BIDS namespace metadata
         path (str) : path of the JSON file
         namespace (str) : key in the meta_info to save (BIDS)
+        outdir: directory where the json will be written
     """
+
     # Remove the 'BIDS' value from info
     try:
-        ns_data = meta_info.pop(namespace)
+        ns_data = metadata.pop(namespace)
     except:
         ns_data = {}
 
     # If meta info is empty, simply return
-    if not meta_info:
+    if not metadata:
+        logger.info(
+            f"No metadata, besides possibly {namespace}, available to create sidecar"
+        )
         return
 
     # If the file is functional,
     #   move the 'TaskName' from 'BIDS'
     #   to the top level
     if "/func/" in path and "Task" in ns_data:
-        meta_info["TaskName"] = ns_data["Task"]
+        metadata["TaskName"] = ns_data["Task"]
 
     # Perform delete and updates
     for key in ns_data.get("delete_info", []):
-        meta_info.pop(key, None)
+        metadata.pop(key, None)
 
     for key, value in ns_data.get("set_info", {}).items():
-        meta_info[key] = value
+        metadata[key] = value
 
     # Remove extension of path and replace with .json
     ext = utils.get_extension(path)
-    new_path = path[: -len(ext)] + ".json"
+    # BIDS filepath is in the namespace_data dict
+    new_path = Path(outdir, ns_data["Path"], Path(path[: -len(ext)]).stem + ".json")
 
     # Write out contents to JSON file
     with open(new_path, "w") as outfile:
-        json.dump(meta_info, outfile, sort_keys=True, indent=4)
+        json.dump(metadata, outfile, sort_keys=True, indent=4)
+    return new_path
 
 
-def download_bids_files(fw, filepath_downloads, dry_run):
+def download_bids_files(fw, filepath_downloads, dry_run, outdir):
     """
     filepath_downloads: {container_type: {filepath: {'args': (tuple of args for sdk download function), 'modified': file modified attr}}}
         args[0] = id
         args[1] = name from the platform
         args[2] = name at dest
         modified = Different, time-based representations of the last changes to the file
     """
+    check_sidecar_exist(fw, filepath_downloads, outdir)
     # Download all project files
     logger.info("Downloading project files")
     for f in filepath_downloads["project"]:
         args = filepath_downloads["project"][f]["args"]
 
-        modified = filepath_downloads["project"][f]["modified"]
-        logger.info(f"Downloading project file: {args[1]}")
-        # For dry run, don't actually download
-        if dry_run:
-            logger.info(f"  to {args[2]}")
-            continue
-        fw.download_file_from_project(*args)
-        # Set the mtime of the downloaded file to the 'modified' timestamp in seconds
-        modified_time = float(timestamp_to_int(modified))
-        os.utime(f, (modified_time, modified_time))
+        try:
+            modified = filepath_downloads["project"][f]["modified"]
+            logger.info(f"Downloading project file: {args[1]}")
+            # For dry run, don't actually download
+            if dry_run:
+                logger.info(f"  to {args[2]}")
+                continue
+            fw.download_file_from_project(*args)
+            # Set the mtime of the downloaded file to the 'modified' timestamp in seconds
+            modified_time = float(timestamp_to_int(modified))
+            os.utime(f, (modified_time, modified_time))
+        except:
+            logger.info(f"{f} not found")
 
         # If zipfile is attached to project, unzip...
         path = args[2]
         zip_pattern = re.compile("[a-zA-Z0-9]+(.zip)")
         zip_dirname = path[:-4]
         if zip_pattern.search(path):
             zip_ref = zipfile.ZipFile(path, "r")
             zip_ref.extractall(zip_dirname)
             zip_ref.close()
             # Remove the zipfile
             os.remove(path)
 
-    for ft in ["session", "acquisition", "sidecars"]:
+    for ft in ["session", "acquisition", "sidecar"]:
         # Download all files for the looped filetype
         logger.info(f"Downloading {ft} files")
         for f in filepath_downloads[ft]:
             args = filepath_downloads[ft][f]["args"]
             logger.info(f"Downloading {ft} file: {args[1]}")
             # For dry run, don't actually download
             if dry_run:
                 logger.info(f"  to {args[2]}")
                 continue
-            if ft == "sidecars":
-                create_json(*args)
+
+            modified = filepath_downloads[ft][f].get("modified")
+            if ft == "sidecar":
+                fw.download_file_from_acquisition(*args)
             else:
-                modified = filepath_downloads[ft][f]["modified"]
                 getattr(fw, "download_file_from_" + ft)(*args)
-                # Set the mtime of the downloaded file to the 'modified' timestamp in seconds
+            # Set the mtime of the downloaded file to the 'modified' timestamp in seconds
+            if modified:
                 modified_time = float(timestamp_to_int(modified))
                 os.utime(f, (modified_time, modified_time))
 
 
 def download_bids_dir(
     fw,
     container_id,
@@ -282,15 +318,15 @@
     is_file_excluded = is_file_excluded_options(namespace, src_data, replace)
 
     # Files and the corresponding download arguments separated by parent container
     filepath_downloads = {
         "project": {},
         "session": {},
         "acquisition": {},
-        "sidecars": {},
+        "sidecar": {},
     }
     valid = True
 
     if container_type == "project":
         # Get project
         project = fw.get_project(container_id)
 
@@ -303,15 +339,14 @@
             )
 
         logger.info("Processing project files")
         # Iterate over any project files
         for f in project.get(
             "files", []
         ):  # Why are jsons excluded from the files list?
-
             # Define path - ensure that the folder exists...
             path = define_path(outdir, f, namespace)
             # If path is not defined (an empty string) move onto next file
             if not path:
                 continue
 
             # Don't exclude any files that specify exclusion
@@ -335,17 +370,22 @@
             filepath_downloads["project"][path] = {
                 "args": (project["_id"], f["name"], path),
                 "modified": f.get("modified"),
             }
 
         ## Create dataset_description.json filepath_download
         path = os.path.join(outdir, "dataset_description.json")
-        filepath_downloads["sidecars"][path] = {
-            "args": (project["info"][namespace], path, namespace)
-        }
+        try:
+            filepath_downloads["project"][path] = {
+                "args": (project["info"][namespace], path, namespace),
+                "modified": f.get("modified"),
+            }
+        except:
+            # ApiException 404 "Detail: Not Found"
+            logger.info(f"dataset_description.json not found.")
         # Get project sessions
         project_sessions = fw.get_project_sessions(container_id)
     elif container_type == "session":
         project_sessions = [fw.get_session(container_id)]
     else:
         project_sessions = []
 
@@ -371,15 +411,14 @@
             if proj_ses.get("files"):
                 session = proj_ses
             else:
                 session = fw.get_session(proj_ses["_id"])
             # Check if session contains files
             # Iterate over any session files
             for f in session.get("files", []):
-
                 # Define path - ensure that the folder exists...
                 path = define_path(outdir, f, namespace)
                 # If path is not defined (an empty string) move onto next file
                 if not path:
                     continue
 
                 # Don't exclude any files that specify exclusion
@@ -418,28 +457,27 @@
             if is_container_excluded(ses_acq, namespace):
                 continue
             # Get true acquisition if files aren't already retrieved, in order to access file info
 
             acq = fw.get_acquisition(ses_acq["_id"])
             # Iterate over acquistion files
             for f in acq.get("files", []):
-
                 # Skip any folders not in the skip-list (if there is a skip list)
                 if folders:
                     folder = get_folder(f, namespace)
                     if folder not in folders:
                         continue
 
                 # Define path - ensure that the folder exists...
                 path = define_path(outdir, f, namespace)
                 # If path is not defined (an empty string) move onto next file
                 if not path:
                     continue
 
-                # Don't exclude any files that specify exclusion
+                # Don't exclude any files that specify exclusion or already exist in the destination folder
                 if is_file_excluded(f, path):
                     continue
 
                 if not os.path.exists(os.path.dirname(path)):
                     os.makedirs(os.path.dirname(path))
 
                 warn_if_bids_invalid(f, namespace)
@@ -447,23 +485,22 @@
                 if path in filepath_downloads["acquisition"]:
                     logger.error(
                         f'Multiple files with path {path}:\n\t{f["name"]} '
                         f'and\n\t{filepath_downloads["acquisition"][path]["args"][1]}'
                     )
                     valid = False
 
-                filepath_downloads["acquisition"][path] = {
+                if ".json" in Path(path).suffix:
+                    file_type = "sidecar"
+                else:
+                    file_type = "acquisition"
+                filepath_downloads[file_type][path] = {
                     "args": (acq["_id"], f["name"], path),
                     "modified": f.get("modified"),
                 }
-
-                # Create the sidecar JSON filepath_download
-                filepath_downloads["sidecars"][path] = {
-                    "args": (f["info"], path, namespace)
-                }
     else:
         msg = (
             container_type
             + ", with subjects="
             + str(subjects)
             + " sessions="
             + str(sessions)
@@ -477,15 +514,15 @@
         # If the BIDS app has its own validation and the user does not want FW to
         # check before downloading the data (validation = False), then don't
         # stop the download and the gear.
         raise BIDSExportError(
             "Error mapping files from Flywheel to BIDS.\n" "Hint: Check curation."
         )
 
-    download_bids_files(fw, filepath_downloads, dry_run)
+    download_bids_files(fw, filepath_downloads, dry_run, outdir)
 
 
 def determine_container(fw, project_label, container_type, container_id, group_id=None):
     """
     Figures out what container_type and container_id should be if not given
     """
     cid = ctype = None
@@ -521,15 +558,14 @@
     replace=False,
     dry_run=False,
     container_type=None,
     container_id=None,
     source_data=False,
     validate=True,
 ):
-
     ### Prep
     # Check directory name - ensure it exists
     validate_dirname(bids_dir)
 
     # Check that container args are valid
     ctype, cid = determine_container(
         fw, project_label, container_type, container_id, group_id=group_id
```

## flywheel_bids/supporting_files/bidsify_flywheel.py

```diff
@@ -94,15 +94,15 @@
 # Accepts a context object that represents a Flywheel container and related parent containers
 # and looks for matching templates in namespace.
 # Matching templates define rules for adding objects to the container's info object if they don't already exist
 # Matching templates with 'auto_update' rules will update existing info object values each time it is run.
 
 
 def process_matching_templates(
-    context: TreeNode, template=templates.DEFAULT_TEMPLATE, upload=False
+    context: TreeNode, template: templates.Template, upload=False
 ):
     """
     Upload or update container following BIDS rules from a template JSON.
 
     Identify whether the container already exists. If not, try to match
     (1) upload_rules, if they exist, (2) rules from the template. Reports
     back if the template is non-existent or rules are not available for the
@@ -136,15 +136,14 @@
         ("info" not in container)
         or (namespace not in container["info"])
         or ("template" not in container["info"][namespace])
     )
 
     # add objects based on template if they don't already exist
     if find_rule_match:
-
         logger.debug(
             f"'info' not in container OR "
             f"{namespace} not in 'info' OR "
             f"'container template' not in info.{namespace}.  "
             f"Performing rule matching\n\n"
         )
 
@@ -230,15 +229,15 @@
         template_properties, match_info, container.get("classification")
     )
     if context["container_type"] in ["session", "acquisition", "file"]:
         match_info["ignore"] = False
     return match_info
 
 
-def process_resolvers(context, template=templates.DEFAULT_TEMPLATE):
+def process_resolvers(context: TreeNode, template: templates.Template):
     """
     Perform second stage path resolution based on template rules
 
     Args:
         session (TreeNode): The session node to search within
         context (dict): The context to perform path resolution on (dictionary of the container with the original, Flywheel info blob)
         template (Template): The template
@@ -259,29 +258,7 @@
     template_name = container["info"][namespace]["template"]
     # Get a list of resolvers that apply to this template
     resolvers = template.resolver_map.get(template_name, [])
 
     # Apply each resolver
     for resolver in resolvers:
         resolver.resolve(context)
-
-
-def ensure_info_exists(context, template=templates.DEFAULT_TEMPLATE):
-    """
-    Ensure that the given info object has an entry for the template namespace.
-
-    Args:
-        context (dict): dictionary of the container with the original, Flywheel info blob
-        template (Template): The template
-
-    Returns:
-        bool: Whether or not the info was updated.
-    """
-    updated = False
-    if "info" not in context:
-        context["info"] = {}
-        updated = True
-    if template.namespace not in context["info"]:
-        context["info"][template.namespace] = {}
-        updated = True
-
-    return updated
```

## flywheel_bids/supporting_files/resolver.py

```diff
@@ -1,7 +1,11 @@
+import json
+
+
+import flywheel
 from . import utils
 
 
 class Filter:
     """
     Simple wrapper for a matching filter that can be applied to a context.
 
@@ -41,33 +45,35 @@
     Args:
         namespace (str): The template namespace
         resolverDef (dict): The resolver properties dictionary
 
     Attributes:
         namespace: The template namespace
         id: The optional resolver id
-        templates: The list of templates this resolver applies to
+        templates: The list of template names (keys in definitions dict) this resolver applies to
         update_field: The field to be updated with the resolved result
         filter_field: The field that contains the user-defined filter
         container_type: The type of container this resolver should match
         resolve_for: The level that resolution for this resolver should take place (e.g. session)
         format: The format string for resolved values
         value: The path to the value to copy, if not using a format string
     """
 
-    def __init__(self, namespace, resolverDef):
+    def __init__(self, namespace, resolverDef, fw, save_sidecar_as_metadata=False):
         self.namespace = namespace
         self.id = resolverDef.get("id")
         self.templates = resolverDef.get("templates", [])
         self.update_field = resolverDef.get("update")
         self.filter_field = resolverDef.get("filter")
         self.container_type = resolverDef.get("type")
         self.resolve_for = resolverDef.get("resolveFor")
         self.format = resolverDef.get("format")
         self.value = resolverDef.get("value")
+        self.save_sidecar_as_metadata = save_sidecar_as_metadata
+        self.client = fw
 
         if self.format and self.value:
             print(
                 'WARNING: Because "format" is specified, "value" will be ignored for resolver: {}'.format(
                     self.id
                 )
             )
@@ -111,8 +117,30 @@
                             if results and results != value:
                                 print(
                                     "WARNING: multiple different matches when resolving results, will take last match!"
                                 )
                             results = value
 
         # Finally update the field specified
-        utils.dict_set(context, self.update_field, results)
+        if self.save_sidecar_as_metadata:  # then update that metadata
+            utils.dict_set(context, self.update_field, results)
+        else:  # using actual sidecar, not metadata, so update that json file
+            # get name of side car
+            nifti_name = context["file"].data["name"]
+            if not nifti_name.endswith(".nii.gz"):
+                print(
+                    f"Unexpected file name '{nifti_name}', should end with '.nii.gz'  "
+                )
+            else:
+                sidecar_name = nifti_name[:-7] + ".json"
+                acquisition = self.client.get_acquisition(
+                    context["acquisition"].data["id"]
+                )
+                sidecar_contents = acquisition.read_file(sidecar_name)
+                if not sidecar_contents:
+                    print(f"Unable to load {sidecar_name}")
+                sidecar_json = json.loads(sidecar_contents)
+                field_name = self.update_field.split(".")[-1]
+                sidecar_json[field_name] = results
+                json_str = json.dumps(sidecar_json, indent=4)
+                file_spec = flywheel.FileSpec(sidecar_name, json_str, "text/plain")
+                acquisition.upload_file(file_spec)
```

## flywheel_bids/supporting_files/templates.py

```diff
@@ -6,120 +6,131 @@
 import re
 
 import jsonschema
 import six
 
 from . import resolver, utils
 
+
+logger = logging.getLogger("curate-bids")
+
 DEFAULT_TEMPLATE_NAME = "default"
 BIDS_V1_TEMPLATE_NAME = "bids-v1"
 REPROIN_TEMPLATE_NAME = "reproin"
-DEFAULT_TEMPLATES = {}
 
-logger = logging.getLogger("curate-bids")
+this_dir = os.path.dirname(os.path.realpath(__file__))
+DEFAULT_TEMPLATE_DIR = os.path.join(this_dir, "../templates")
+
+
+def load_and_normalize_json(path):
+    with open(path, "r") as f:
+        data = json.load(f)
+    return utils.normalize_strings(data)
 
 
 class Template:
     """
     Represents a project-level template for organizing data.
 
     Args:
         data (dict): The json configuration for the template
-        templates (list): The optional existing map of templates by name.
 
     Attributes:
         namespace (str): The namespace where resolved template data is displayed.
         description (str): The optional description of the template.
         definitions (dict): The map of template definitions.
         rules (list): The list of if rules for applying templates.
         extends (string): The optional name of the template to extend.
         exclude_rules (list): The optional list of rules to exclude from a parent template.
     """
 
-    def __init__(self, data, templates=None):
+    def __init__(self, data, fw=None, save_sidecar_as_metadata=False):
         if data:
             self.namespace = data.get("namespace")
             self.description = data.get("description", "")
             self.definitions = data.get("definitions", {})
             self.rules = data.get("rules", [])
             self.upload_rules = data.get("upload_rules", [])
             self.resolvers = data.get("resolvers", [])
             self.custom_initializers = data.get("initializers", [])
-
             self.extends = data.get("extends")
             self.exclude_rules = data.get("exclude_rules", [])
         else:
             raise Exception("data is required")
 
-        if templates:
-            self.do_extend(templates)
+        if self.extends:
+            self.do_extend(fw, save_sidecar_as_metadata)
 
         resolver = jsonschema.RefResolver.from_schema({"definitions": self.definitions})
         self.resolve_refs(resolver, self.definitions)
-        self.compile_resolvers()
+        self.compile_resolvers(fw, save_sidecar_as_metadata)
         self.compile_rules()
         self.compile_custom_initializers()
 
-    def do_extend(self, templates):
+    def do_extend(self, fw, save_sidecar_as_metadata):
         """
-        Implements the extends logic for this template.
-
-        Args:
-            templates (list): The existing list of templates.
+        The template (json file) just read is an extension template so load the template
+        that is to be extended (one of the defined templates).
         """
-        if not self.extends:
-            return
 
-        if self.extends not in templates:
-            raise Exception("Could not find parent template: {0}".format(self.extends))
+        logger.info("Extending project curation template: '%s'", self.extends)
 
-        parent = templates[self.extends]
+        parent = load_template(
+            fw,
+            template_name=self.extends,
+            save_sidecar_as_metadata=save_sidecar_as_metadata,
+        )
 
         if not self.namespace:
             self.namespace = parent.namespace
 
         my_rules = self.rules
         my_defs = self.definitions
         my_resolvers = self.resolvers
+        my_initializers = self.custom_initializers
 
         # Extend definitions
         self.definitions = parent.definitions.copy()
         for key, value in my_defs.items():
             self.definitions[key] = value
 
         # Extend rules, after filtering excluded rules
         filtered_rules = filter(lambda x: x.id not in self.exclude_rules, parent.rules)
         self.rules = my_rules + list(filtered_rules)
 
         # Extend resolvers
         self.resolvers = my_resolvers + parent.resolvers
 
+        self.custom_initializers = my_initializers + parent.custom_initializers
+
     def compile_rules(self):
         """
         Converts the rule dictionaries on this object to Rule class objects.
         """
         for i in range(0, len(self.rules)):
             rule = self.rules[i]
             if not isinstance(rule, Rule):
                 self.rules[i] = Rule(rule)
 
         for i in range(0, len(self.upload_rules)):
             upload_rule = self.upload_rules[i]
             if not isinstance(upload_rule, Rule):
                 self.upload_rules[i] = Rule(upload_rule)
 
-    def compile_resolvers(self):
+    def compile_resolvers(self, fw=None, save_sidecar_as_metadata=False):
         """
         Walk through the definitions
         """
         self.resolver_map = {}
         for i in range(0, len(self.resolvers)):
             res = self.resolvers[i]
             if not isinstance(res, resolver.Resolver):
-                res = resolver.Resolver(self.namespace, res)
+                res = resolver.Resolver(
+                    self.namespace, res, fw, save_sidecar_as_metadata
+                )
 
             # Create a mapping of template id to resolver
             for tmpl in res.templates:
                 if tmpl not in self.resolver_map:
                     self.resolver_map[tmpl] = []
                 self.resolver_map[tmpl].append(res)
 
@@ -587,55 +598,31 @@
         return any(match_values)
     elif "and" in condition:
         return all(match_values)
     else:
         logger.error("Expected or/and not %s", condition)
 
 
-def loadTemplates(templates_dir=None):
-    """
-    Load all templates in the given (or default) directory
-
-    Args:
-        templates_dir (string): The optional directory to load templates from.
-    """
-    results = {}
-
-    if templates_dir is None:
-        script_dir = os.path.dirname(os.path.realpath(__file__))
-        templates_dir = os.path.join(script_dir, "../templates")
-
-    # Load all templates from the templates directory
-    for fname in os.listdir(templates_dir):
-        path = os.path.join(templates_dir, fname)
-        name, ext = os.path.splitext(fname)
-        if ext == ".json" and os.path.isfile(path):
-            results[name] = loadTemplate(path)
-
-    return results
-
-
-def loadTemplate(path, templates=None):
+def load_template(fw, path=None, template_name=None, save_sidecar_as_metadata=False):
     """
-    Load the template at path
+    Load the template at path or the named template at the default path
 
     Args:
+        fw (Flywheel Client)
         path (str): The path to the template to load
-        templates (dict): The mapping of template names to template defintions.
+        template_name (str): will load default_path + / + template_name + .json
+        save_sidecar_as_metadata (bool): sidecar info is stored in metadata not sidecar file
     Returns:
         Template: The template that was loaded (otherwise throws)
     """
-    with open(path, "r") as f:
-        data = json.load(f)
 
-    data = utils.normalize_strings(data)
+    if path is None:
+        if template_name is None:
+            template_name = DEFAULT_TEMPLATE_NAME
 
-    if templates is None:
-        templates = DEFAULT_TEMPLATES
+        path = os.path.join(DEFAULT_TEMPLATE_DIR, template_name + ".json")
 
-    return Template(data, templates)
+    logger.info("Using project curation template: '%s'", path)
 
+    data = load_and_normalize_json(path)
 
-DEFAULT_TEMPLATES = loadTemplates()
-DEFAULT_TEMPLATE = DEFAULT_TEMPLATES.get(DEFAULT_TEMPLATE_NAME)
-BIDS_V1_TEMPLATE = DEFAULT_TEMPLATES.get(BIDS_V1_TEMPLATE_NAME)
-REPROIN_TEMPLATE = DEFAULT_TEMPLATES.get(REPROIN_TEMPLATE_NAME)
+    return Template(data, fw, save_sidecar_as_metadata)
```

## flywheel_bids/templates/reproin.json

### Pretty-printed

 * *Similarity: 0.9943395543981483%*

 * *Differences: {"'definitions'": "{'perf_tsv_file': {'properties': {delete: ['required']}, 'required': "*

 * *                  "['Filename', 'Task', 'Modality']}}",*

 * * "'rules'": "{insert: [(5, OrderedDict([('id', 'dataset_description_json'), ('template', "*

 * *            "'project_file'), ('where', OrderedDict([('container_type', 'file'), "*

 * *            "('parent_container_type', 'project'), ('file.type', OrderedDict([('$in', ['source "*

 * *            "code', 'JSON'])]))])), ('initialize', OrderedDict([('Filename', "*

 * *            "Ord [â€¦]*

```diff
@@ -594,21 +594,21 @@
                     "$ref": "#/definitions/Recording"
                 },
                 "Run": {
                     "$ref": "#/definitions/Run"
                 },
                 "Task": {
                     "$ref": "#/definitions/Task"
-                },
-                "required": [
-                    "Filename",
-                    "Task",
-                    "Modality"
-                ]
-            }
+                }
+            },
+            "required": [
+                "Filename",
+                "Task",
+                "Modality"
+            ]
         },
         "physio_task_file": {
             "description": "BIDS physio file template",
             "properties": {
                 "Acq": {
                     "$ref": "#/definitions/Acq"
                 },
@@ -1014,14 +1014,35 @@
             },
             "template": "acquisition",
             "where": {
                 "container_type": "acquisition"
             }
         },
         {
+            "id": "dataset_description_json",
+            "initialize": {
+                "Filename": {
+                    "file.name": {
+                        "$take": true
+                    }
+                }
+            },
+            "template": "project_file",
+            "where": {
+                "container_type": "file",
+                "file.type": {
+                    "$in": [
+                        "source code",
+                        "JSON"
+                    ]
+                },
+                "parent_container_type": "project"
+            }
+        },
+        {
             "id": "reproin_anat_file",
             "initialize": {
                 "Acq": {
                     "acquisition.label": {
                         "$regex": "(^|_)acq-(?P<value>.*?)(_(acq|ce|dir|echo|mod|proc|part|rec|recording|run|task)-|$|_)"
                     }
                 },
```

## flywheel_bids/upload_bids.py

```diff
@@ -7,33 +7,38 @@
 import shutil
 import sys
 
 import flywheel
 from six.moves import reduce
 
 from flywheel_bids.supporting_files import bidsify_flywheel, classifications, utils
-from flywheel_bids.supporting_files.templates import DEFAULT_TEMPLATE as template
+from flywheel_bids.supporting_files.templates import load_template, Template
 
 SECONDS_PER_YEAR = 86400 * 365.25
 
 logging.basicConfig(level=logging.INFO)
 logger = logging.getLogger("bids-uploader")
 
+
 ####### Alphabetized, helper methods ########
-def attach_json(fw, file_info):
+def attach_json(fw, file_info, save_sidecar_as_metadata=False, overwrite=False):
     # Parse JSON file
     contents = parse_json(file_info["full_filename"])
     # Attach parsed JSON to project
     if "dataset_description.json" in file_info["full_filename"]:
-        proj = fw.get_project(file_info["id"]).to_dict()
-        proj.get("info").get(template.namespace).update(contents)
-        fw.modify_project(
-            file_info["id"],
-            {"info": {template.namespace: proj.get("info").get(template.namespace)}},
-        )
+        proj = fw.get_project(file_info["id"])
+        if file_does_not_exist(proj.files, file_info["full_filename"]) or overwrite:
+            proj.upload_file(file_info["full_filename"])
+        # Following lines add the json contents as metadata
+        # proj = fw.get_project(file_info["id"]).to_dict()
+        # proj.get("info").get(template_namespace).update(contents)
+        # fw.modify_project(
+        #    file_info["id"],
+        #    {"info": {template_namespace: proj.get("info").get(template_namespace)}},
+        # )
     # Otherwise... it's a JSON file that should be assigned to acquisition file(s)
     else:
         # Figure out which acquisition files within PROJECT should have JSON info attached...
         if file_info["id_type"] == "project":
             # Get sessions within project
             proj_sess = [s.to_dict() for s in fw.get_project_sessions(file_info["id"])]
             for proj_ses in proj_sess:
@@ -41,52 +46,90 @@
                 ses_acqs = [
                     a.to_dict() for a in fw.get_session_acquisitions(proj_ses["id"])
                 ]
                 for ses_acq in ses_acqs:
                     # Iterate over every acquisition file
                     for f in ses_acq["files"]:
                         # Determine if json file components are all within the acq filename
-                        if compare_json_to_file(
-                            os.path.basename(file_info["full_filename"]), f["name"]
+                        if (
+                            compare_json_to_file(
+                                os.path.basename(file_info["full_filename"]), f["name"]
+                            )
+                            and save_sidecar_as_metadata
                         ):
                             # JSON matches to file - assign json contents as file meta info
                             f["info"].update(contents)
                             fw.set_acquisition_file_info(
                                 ses_acq["id"], f["name"], f["info"]
                             )
+                        elif compare_json_to_file(
+                            os.path.basename(file_info["full_filename"]), f["name"]
+                        ) and (
+                            file_does_not_exist(
+                                ses_acq["files"], file_info["full_filename"]
+                            )
+                            or overwrite
+                        ):
+                            fw.upload_file_to_session(
+                                ses_acq["id"], file_info["full_filename"]
+                            )
 
         # Figure out which acquisition files within SESSION should have JSON info attached...
         elif file_info["id_type"] == "session":
             # Get session and iterate over every acquisition file
             ses_acqs = [
                 a.to_dict() for a in fw.get_session_acquisitions(file_info["id"])
             ]
             for ses_acq in ses_acqs:
                 for f in ses_acq["files"]:
                     # Determine if json file components are all within the acq filename
-                    if compare_json_to_file(
-                        os.path.basename(file_info["full_filename"]), f["name"]
+                    if (
+                        compare_json_to_file(
+                            os.path.basename(file_info["full_filename"]), f["name"]
+                        )
+                        and save_sidecar_as_metadata
                     ):
                         # JSON matches to file - assign json contents as file meta info
                         f["info"].update(contents)
                         fw.set_acquisition_file_info(
                             ses_acq["id"], f["name"], f["info"]
                         )
+                    elif compare_json_to_file(
+                        os.path.basename(file_info["full_filename"]), f["name"]
+                    ) and (
+                        file_does_not_exist(
+                            ses_acq["files"], file_info["full_filename"]
+                        )
+                        or overwrite
+                    ):
+                        fw.upload_file_to_session(
+                            ses_acq["id"], file_info["full_filename"]
+                        )
 
         # Figure out which acquisition files within ACQUISITION should have JSON info attached...
         elif file_info["id_type"] == "acquisition":
             acq = fw.get_acquisition(file_info["id"]).to_dict()
             for f in acq["files"]:
                 # Determine if json file components are all within the acq filename
-                if compare_json_to_file(
-                    os.path.basename(file_info["full_filename"]), f["name"]
+                if (
+                    compare_json_to_file(
+                        os.path.basename(file_info["full_filename"]), f["name"]
+                    )
+                    and save_sidecar_as_metadata
                 ):
                     # JSON matches to file - assign json contents as file meta info
                     f["info"].update(contents)
                     fw.set_acquisition_file_info(acq["id"], f["name"], f["info"])
+                elif compare_json_to_file(
+                    os.path.basename(file_info["full_filename"]), f["name"]
+                ) and (
+                    file_does_not_exist(acq["files"], file_info["full_filename"])
+                    or overwrite
+                ):
+                    fw.upload_file_to_acquisition(acq["id"], file_info["full_filename"])
 
 
 def attach_project_tsv(fw, project_id, info_rows):
     # Get sessions within project
     sessions = [s.to_dict() for s in fw.get_project_sessions(project_id)]
     sessions_by_code = {}
 
@@ -161,15 +204,15 @@
                     session_info[key] = int(value * SECONDS_PER_YEAR)
                 elif key != "session_id":
                     session_info["info"][key] = value
 
             fw.modify_session(ses["id"], session_info)
 
 
-def attach_tsv(fw, file_info):
+def attach_tsv(fw, file_info, overwrite=False):
     ## Parse TSV file
     contents = parse_tsv(file_info["full_filename"])
 
     ## Attach TSV file contents
     # Get headers of the TSV file
     headers = contents[0]
 
@@ -214,14 +257,31 @@
 def check_enabled_rules(fw, project_id):
     for rule in fw.get_project_rules(project_id):
         if not rule.get("disabled"):
             return True
     return False
 
 
+def check_modality_supported(modality):
+    """Check if the modality folder (i.e., 'anat') is in the dictionary of supported
+    modality types. Only keys in the dictionary have been defined in the template."""
+    if any(substr in modality for substr in classifications.classifications.keys()):
+        return True
+    elif "." not in modality:
+        logger.warning(
+            f"{modality} is not yet supported for BIDS import.\n"
+            f"Expecting one of the following modalities as the folder name:\n"
+            f"{classifications.classifications.keys()}"
+        )
+        return False
+    else:
+        # Has warning from previous search; silent return
+        return False
+
+
 def classify_acquisition(full_fname):
     """Return classification of file based on filename"""
 
     # Get the folder and filename from the full filename
     full_fname = os.path.normpath(full_fname)
     parts = full_fname.split(os.sep)
     folder = parts[-2]
@@ -369,14 +429,15 @@
         acq_label = foldername
     else:
         # Get acq_label from file basename
         #  remove extension from the filename
         fname = fname.split(".")[0]
         #  split up filename into parts, removing the final part, the Modality
         parts = fname.split("_")
+
         # reorder name parts and handle modality-specific cases
         if not hasattr(sys.modules[__name__], "reproin_%s" % foldername):
             foldername = classifications.determine_modality(foldername)
 
         if hasattr(sys.modules[__name__], "reproin_%s" % foldername):
             parts = getattr(sys.modules[__name__], "reproin_%s" % foldername)(parts)
             # Ensure no dups, as would happen, if the orig input was already reproin
@@ -425,14 +486,24 @@
             return False
         else:
             return True
     else:
         return True
 
 
+def file_does_not_exist(filelist, filename):
+    """Before overwriting, check for file."""
+    if os.sep in filename:
+        filename = os.path.basename(filename)
+    try:
+        return any([f for f in filelist if filename in f.name])
+    except AttributeError:
+        return any([f for f in filelist if filename in f["name"]])
+
+
 def fill_in_properties(
     bc_context, path, local_properties: bool, corrected_dir_name: str = None
 ):
     """ """
     # Define the regex to use to find the property value from filename
     properties_regex = {
         "Acq": "_acq-[a-zA-Z0-9]+",
@@ -446,37 +517,35 @@
         "Recording": "_recording-[a-zA-Z0-9]+",
         "Modality": "_[a-zA-Z0-9]+%s" % bc_context["ext"],
     }
     path = os.path.normpath(path)
     path, correct_bids_dir = check_bids_dir_name(path)
     # Get meta info
     meta_info = bc_context["file"]["info"]
-    # Get namespace
-    namespace = template.namespace
     # Iterate over all of the keys within the info namespace ('BIDS')
-    for mi in meta_info[namespace]:
-        if not local_properties and meta_info[namespace][mi]:
+    for mi in meta_info["BIDS"]:
+        if not local_properties and meta_info["BIDS"][mi]:
             continue
         elif mi == "Filename":
-            meta_info[namespace][mi] = bc_context["file"]["name"]
+            meta_info["BIDS"][mi] = bc_context["file"]["name"]
         elif mi == "Folder":
             if "sourcedata" in path:
-                meta_info[namespace][mi] = "sourcedata"
+                meta_info["BIDS"][mi] = "sourcedata"
             elif "derivatives" in path:
-                meta_info[namespace][mi] = "derivatives"
+                meta_info["BIDS"][mi] = "derivatives"
             else:
-                meta_info[namespace][mi] = correct_bids_dir
+                meta_info["BIDS"][mi] = correct_bids_dir
         elif mi == "Path":
             if corrected_dir_name:
                 # for cases of blah/blah/anat-extra_stuff
-                meta_info[namespace][mi] = os.path.join(
+                meta_info["BIDS"][mi] = os.path.join(
                     path.split(os.sep)[:-1], corrected_dir_name
                 )
             else:
-                meta_info[namespace][mi] = path
+                meta_info["BIDS"][mi] = path
             # Search for regex string within BIDS filename and populate meta_info
         elif mi in properties_regex:
             tokens = re.compile(properties_regex[mi])
             token = tokens.search(bc_context["file"]["name"])
             if token:
                 # Get the matched string
                 result = token.group()
@@ -486,15 +555,15 @@
                 # Get the value after the '-'
                 else:
                     value = result.split("-")[-1]
                 # If value as an 'index' instead of a 'label', make it an integer (for search)
                 if mi in ["Run", "Echo"]:
                     value = str(value)
                 # Assign value to meta_info
-                meta_info[namespace][mi] = value
+                meta_info["BIDS"][mi] = value
     return meta_info
 
 
 def handle_acquisition(fw, session_id, acquisition_label, subject_name):
     """Retrieve or create an acquisition from/on Flywheel.
 
     :param fw: Flywheel client
@@ -527,19 +596,19 @@
         )
         acquisition_id = fw.add_acquisition(
             {"label": acquisition_label, "session": session_id}
         )
         acquisition = fw.get_acquisition(acquisition_id)
 
     # In either case, check if there is BIDS information
-    if not hasattr(acquisition["info"], template.namespace):
+    if not hasattr(acquisition["info"], "BIDS"):
         acquisition.update(
             {
                 "info": {
-                    template.namespace: {
+                    "BIDS": {
                         "Subject": subject_name[4:],
                         "Label": acquisition_label,
                         "ignore": False,
                     }
                 }
             }
         )
@@ -633,15 +702,15 @@
 
         session_id = fw.add_session(
             {
                 "label": session_name,
                 "project": project_id,
                 "subject": {"code": subject_name},
                 "info": {
-                    template.namespace: {
+                    "BIDS": {
                         "Subject": subject_name[4:],
                         "Label": session_name[4:],
                         "ignore": False,
                     }
                 },
             }
         )
@@ -685,14 +754,15 @@
     bc_context,
     files_for_special_handling,
     subject,
     rootdir,
     sub_rootdir,
     hierarchy_type,
     subject_code,
+    template: Template,
     local_properties: bool,
     overwrite=False,
 ):
     #   In BIDS, the session is optional, if not present - use subject_code as session_label
     # Get all keys that are session - 'ses-<session.label>'
     if sub_rootdir:
         rootdir = os.path.join(rootdir, sub_rootdir)
@@ -802,14 +872,23 @@
                 }
 
         ## Iterate over 'folders' which are ['anat', 'func', 'fmap', 'dwi'...]
         #          NOTE: could there be any other dirs that would be handled differently?
         # get folders
         folders = [item for item in subject[session_label] if item != "files"]
         for foldername in folders:
+            # Check as early as possible whether the modality is supported.
+            if not check_modality_supported(foldername):
+                # Just in case the BIDS mod is in the filename of the acq, not the folder name
+                deeper_modality_check = subject[session_label][foldername].get("files")[
+                    0
+                ]
+                if not check_modality_supported(deeper_modality_check):
+                    break
+
             # Iterate over acquisition files -- upload file and add meta data
             for fname in subject[session_label][foldername].get("files"):
                 # Exclude filenames that begin with .
                 if fname.startswith("."):
                     continue
                 # Determine acquisition label -- it can either be the folder name OR the basename of the file...
                 acq_label = determine_acquisition_label(
@@ -1022,14 +1101,28 @@
 
 
 def parse_bids_dir(bids_dir):
     """
     Creates a nested dictionary that represents the folder structure of bids_dir
 
     if '/tmp/ds001' is bids dir passed, 'ds001' is first key and is the project name...
+    e.g.,
+    tmp
+    |---ds001
+    |---|--dataset_description.json
+    |---|--anat
+    |---|--|-example_t1.nii.gz
+    |---|--func
+    |---|--|-example_epi.nii.gz
+    |---|--|-example_epi.json
+    becomes {'ds001':{'files':['dataset_description.json'],
+                   'anat':{'files':['example_t1.nii.gz']},
+                   'func':{'files':['example_epi_nii.gz','example_epi.json']}
+                   }
+            }
     """
     ## Read in BIDS hierarchy
     bids_hierarchy = {}
     bids_dir = bids_dir.rstrip(os.sep)
     start = bids_dir.rfind(os.sep) + 1
     for path, dirs, files in os.walk(bids_dir):
         folders = path[start:].split(os.sep)
@@ -1155,14 +1248,15 @@
 
 def upload_bids_dir(
     fw,
     bids_hierarchy: dict,
     group_id: str,
     rootdir: os.PathLike,
     hierarchy_type: str,
+    template: Template,
     local_properties: bool,
     assume_upload: bool,
     overwrite: bool,
 ):
     """
 
     :param fw: Flywheel client
@@ -1210,21 +1304,15 @@
             continue
         # NOTE: template is the default.json file
         bc_context["project"] = bidsify_flywheel.process_matching_templates(
             bc_context, template, upload=True
         )
         fw.modify_project(
             bc_context["project"]["id"],
-            {
-                "info": {
-                    template.namespace: bc_context["project"]["info"][
-                        template.namespace
-                    ]
-                }
-            },
+            {"info": {"BIDS": bc_context["project"]["info"]["BIDS"]}},
         )
 
         ### Iterate over project files - upload file and add metadata
         # Specifically looking for dataset_description.json etc.
         for fname in bids_hierarchy[proj_label].get("files"):
             # Exclude filenames that begin with .
             if fname.startswith("."):
@@ -1333,14 +1421,15 @@
                 bc_context,
                 files_for_special_handling,
                 subject,
                 rootdir,
                 "",
                 hierarchy_type,
                 subject_code,
+                template,
                 local_properties,
                 overwrite,
             )
 
         # upload sourcedata (If option not set, the folder was popped in parse_hierarchy)
         for subject_code in sourcedata_folder:
             subject = bids_hierarchy[proj_label]["sourcedata"][subject_code]
@@ -1349,22 +1438,25 @@
                 bc_context,
                 files_for_special_handling,
                 subject,
                 rootdir,
                 "sourcedata",
                 hierarchy_type,
                 subject_code,
+                template,
                 local_properties,
                 overwrite,
             )
 
     return files_for_special_handling
 
 
-def parse_meta_files(fw, files_for_special_handling):
+def parse_meta_files(
+    fw, files_for_special_handling, save_sidecar_as_metadata=False, overwrite=False
+):
     """
 
     i.e.
 
     files_for_special_handling = {
         'dataset_description.json': {
             'id': u'5a1364af9b89b7001d1f357f',
@@ -1387,18 +1479,23 @@
     """
     logger.info("Parsing meta files")
 
     # Handle files
     for f in files_for_special_handling:
         if ".tsv" in f:
             # Attach TSV file contents
-            attach_tsv(fw, files_for_special_handling[f])
+            attach_tsv(fw, files_for_special_handling[f], overwrite=overwrite)
         elif ".json" in f:
             # Attach JSON file contents
-            attach_json(fw, files_for_special_handling[f])
+            attach_json(
+                fw,
+                files_for_special_handling[f],
+                save_sidecar_as_metadata=save_sidecar_as_metadata,
+                overwrite=overwrite,
+            )
         # Otherwise don't recognize filetype
         else:
             logger.info("Do not recognize filetype")
 
 
 def upload_bids(
     fw,
@@ -1409,14 +1506,17 @@
     validate=True,
     include_source_data=False,
     local_properties=True,
     assume_upload=False,
     subject_label=None,
     session_label=None,
     overwrite=False,
+    save_sidecar_as_metadata=False,
+    template_path=None,
+    template_name=None,
 ):
     ### Prep
     # Check directory name - ensure it exists
     validate_dirname(bids_dir)
 
     ### Read in hierarchy & Validate as BIDS
     # parse BIDS dir; returns dictionary of the hierarchy with
@@ -1433,30 +1533,40 @@
         session_label,
     )
 
     # Determine if hierarchy is valid BIDS
     if validate:
         utils.validate_bids(rootdir)
 
+    template = load_template(
+        fw, template_path, template_name, save_sidecar_as_metadata=False
+    )
+
     ### Upload BIDS directory
     # upload bids dir (and get files of interest and project id)
     files_for_special_handling = upload_bids_dir(
         fw,
         bids_hierarchy,
         group_id,
         rootdir,
         hierarchy_type,
+        template,
         local_properties,
         assume_upload,
         overwrite,
     )
 
     # Parse the BIDS meta files
     #    data_description.json, participants.tsv, *_sessions.tsv, *_scans.tsv
-    parse_meta_files(fw, files_for_special_handling)
+    parse_meta_files(
+        fw,
+        files_for_special_handling,
+        save_sidecar_as_metadata=save_sidecar_as_metadata,
+        overwrite=overwrite,
+    )
 
 
 def main():
     ### Read in arguments
     parser = argparse.ArgumentParser(description="BIDS Directory Upload")
     parser.add_argument(
         "--bids-dir",
@@ -1524,17 +1634,40 @@
         action="store_true",
         help="Assume the answer is yes to all prompts",
     )
     parser.add_argument(
         "-o",
         "--overwrite",
         default=False,
-        requirec=False,
+        required=False,
         help="Existing files should be replaced. (default = false)",
     )
+    parser.add_argument(
+        "--save_sidecar_as_metadata",
+        default=False,
+        required=False,
+        help="Upload the BIDS sidecar as metadata in file.info. (default = false)",
+    )
+    parser.add_argument(
+        "--template-path",
+        dest="template_path",
+        action="store",
+        required=False,
+        default=None,
+        help="Full path to project curation template (.json file)",
+    )
+    parser.add_argument(
+        "--template-name",
+        dest="template_name",
+        action="store",
+        required=False,
+        default="default",
+        choices=["default", "bids-v1", "reproin"],
+        help="Project curation template, either 'default', 'bids-v1' or 'reproin'",
+    )
     args = parser.parse_args()
 
     if args.session and not args.subject:
         logger.error("Cannot only provide session without subject")
         sys.exit(1)
 
     # Check API key - raises Error if key is invalid
@@ -1548,12 +1681,15 @@
         hierarchy_type=args.hierarchy_type,
         include_source_data=args.source_data,
         local_properties=args.local_properties,
         assume_upload=args.yes,
         subject_label=args.subject,
         session_label=args.session,
         overwrite=args.overwrite,
+        save_sidecar_as_metadata=args.save_sidecar_as_metadata,
+        template_path=args.template_path,
+        template_name=args.template_name,
     )
 
 
 if __name__ == "__main__":
     main()
```

## Comparing `flywheel_bids-1.1.5.dist-info/METADATA` & `flywheel_bids-1.2.0.dist-info/METADATA`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: flywheel-bids
-Version: 1.1.5
+Version: 1.2.0
 Summary: Flywheel BIDS Client
 Home-page: https://gitlab.com/flywheel-io/public/bids-client
 License: MIT
 Keywords: Flywheel,flywheel,BIDS,SDK
 Author: Flywheel
 Author-email: support@flywheel.io
 Requires-Python: >=3.8,<4.0
@@ -15,14 +15,15 @@
 Classifier: Programming Language :: Python :: 3.10
 Classifier: Programming Language :: Python :: 3.11
 Classifier: Topic :: Scientific/Engineering
 Requires-Dist: flywheel-gear-toolkit (>=0.6.0,<0.7.0)
 Requires-Dist: flywheel-sdk (>=15.8.0,<16.0.0) ; python_full_version < "3.8.0"
 Requires-Dist: flywheel-sdk (>=16.8.0,<17.0.0) ; python_full_version >= "3.8.0"
 Requires-Dist: future (>=0.18.2)
+Requires-Dist: importlib-resources (>=5.12.0,<6.0.0)
 Requires-Dist: jsonschema (>=3.2.0)
 Requires-Dist: rtstatlib (>=1.0.0)
 Requires-Dist: urllib3 (>=1.26.4)
 Project-URL: Repository, https://gitlab.com/flywheel-io/public/bids-client
 Description-Content-Type: text/markdown
 
 <!-- markdownlint-configure-file { "MD024": { "siblings_only": true } } -->
```

## Comparing `flywheel_bids-1.1.5.dist-info/LICENSE` & `flywheel_bids-1.2.0.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `flywheel_bids-1.1.5.dist-info/RECORD` & `flywheel_bids-1.2.0.dist-info/RECORD`

 * *Files 13% similar despite different names*

```diff
@@ -1,21 +1,21 @@
 flywheel_bids/__init__.py,sha256=Y4xogqvLXa5sAUBRzU_qrcOE145CqsQhbeTjpwPmX1E,5
-flywheel_bids/curate_bids.py,sha256=3KOgB_SHaNYFPqReFFZkOdiRM9X6pem7qG3tM-1xj7o,22103
-flywheel_bids/export_bids.py,sha256=8-ctS2z7p14xd7yUGlYaARLzO3WMNPnQePy0WBIVLro,21920
+flywheel_bids/curate_bids.py,sha256=otrXE681sM7dcSDkPieBCKufKcpm2TcNuZ183cR8d20,22562
+flywheel_bids/export_bids.py,sha256=Xz9Lms7hGaHxZRvA7rMqjmNVwDkNiJFN09D50CENt6Q,23671
 flywheel_bids/results/__init__.py,sha256=enUMUGNyNRTT1jGvzqiPUAO0b7akF5ybOwmdvELPIbU,76
 flywheel_bids/results/zip_htmls.py,sha256=LcntrHonTlUerhwxZ1SlG--5JMHeIvh1d0dvd2tOUfY,2361
 flywheel_bids/results/zip_intermediate.py,sha256=DZmSBnkj_YdpRKfZC13MimwjGjwclWDXF06uv3uMS9c,5147
 flywheel_bids/supporting_files/__init__.py,sha256=Y4xogqvLXa5sAUBRzU_qrcOE145CqsQhbeTjpwPmX1E,5
-flywheel_bids/supporting_files/bidsify_flywheel.py,sha256=HdhWHH_O4Am8EyK-cibuYAVeym-WgvGPbFs5oVTtbA0,11161
+flywheel_bids/supporting_files/bidsify_flywheel.py,sha256=_JAy7mZOM5BrMl-TMvSs7gnr-lrkfCzI33yo8EvMoH4,10537
 flywheel_bids/supporting_files/classifications.py,sha256=uisUcFW-36Zq4uXFnd-sI1Q93E9E6zgeBqipeF0AdtA,4353
 flywheel_bids/supporting_files/errors.py,sha256=6oi6hA2EM7b-7RiOjCUL0lnVLxOuEclzfoyEvtuaR3o,1234
 flywheel_bids/supporting_files/file_funcs.py,sha256=0ZP4w7Ey5uc8MpeEOgOUjZVNOum05C9Ha-V-17XOt1I,6607
 flywheel_bids/supporting_files/project_tree.py,sha256=081OxA5UgL4l1D0rCa1xQWxuu5Bxsepcdha2ir96E7E,7311
-flywheel_bids/supporting_files/resolver.py,sha256=LioSEPVRexpkhbL9fATHCQUOMoCp4i4lLKYyuYsdegY,4208
-flywheel_bids/supporting_files/templates.py,sha256=x5xXBpQEHAM71ZsdXsg3HlMs6SI7eh5-RSe8tE74i30,23580
+flywheel_bids/supporting_files/resolver.py,sha256=h1vcGoD9iQAcEvKpSvKi8TZ6_KwCMUXTD6b7zWnbuLU,5566
+flywheel_bids/supporting_files/templates.py,sha256=7MW7Y41Q1fBaATkcK2Sl8bC_9zracyzFjgn4EjzOW-g,23538
 flywheel_bids/supporting_files/utils.py,sha256=apYF0J9yp7vqWmsnMCPZb59m2oPOEld_YbKjYDjRoQM,10872
 flywheel_bids/templates/README.md,sha256=W2b6jw4TIpoSxEUqdC8AKjFUWLiTX4Q2kDDCOKK1vaw,175
 flywheel_bids/templates/bids-v1.json,sha256=nxiQHfvfN5tEMTn0USoijkFqixnBGhHPpItiUoJ0qt8,27899
 flywheel_bids/templates/default.json,sha256=dLcG0wzXyL33cOnFT3d6m-OVgo8XCQLXDBmpy8KDTTo,53901
 flywheel_bids/templates/flywheel_curated/README.md,sha256=k290CuURkHP3w847_qKcVwaqVKTKSZv_r8onr2_peDc,1536
 flywheel_bids/templates/flywheel_curated/bridge-bdlong-project-template-attributes.txt,sha256=PsL48bjRmfcVKQR_rU6b9pshg_qnPNpUL7Ls5zIFH8o,271
 flywheel_bids/templates/flywheel_curated/bridge-bdlong-project-template.json,sha256=6tHxHSwRaaoNKocZh594y4QAYOP-WycYPcaeA05opTo,38666
@@ -37,18 +37,18 @@
 flywheel_bids/templates/flywheel_curated/columbia-tottenham-pacct-project-template.json,sha256=wjIDWzItNsns_-06t3jGIiPriibbqAhC-wQfwYw0r0Y,6031
 flywheel_bids/templates/flywheel_curated/nyu-bair-vfs-project-template-attributes.txt,sha256=R5yNRilqy5qdQSGpiywv-WiPI4yVHtPYgkkAuh41hQ8,264
 flywheel_bids/templates/flywheel_curated/nyu-bair-vfs-project-template.json,sha256=6hvJbePJv3h5QW7I54nCXXSC8__4gEZq5aNAxUL_kzo,8483
 flywheel_bids/templates/flywheel_curated/reproin-philips-project-template-attributes.txt,sha256=t7grdbW4tPwepjx45NqTEYj6I_06BpCm0i1X2yilQP0,276
 flywheel_bids/templates/flywheel_curated/reproin-philips-project-template.json,sha256=VKm1m6LODdCrR7zS7TdI-S0yfnAzHvR8mRy1IR8PfC8,30276
 flywheel_bids/templates/flywheel_curated/reproin-reproin-project-template-attributes.txt,sha256=-0swDn_XGInpaYBUeY-ZcWx03WpQUQmhCPuCjEBtfhQ,269
 flywheel_bids/templates/flywheel_curated/reproin-reproin-project-template.json,sha256=v1PhSEOBGDA128GzKzBrWrYLTNZ5Z2tnP7B9vz9ZRrQ,42094
-flywheel_bids/templates/reproin.json,sha256=ojvP6mf6ZtnYMQesW8WqfdfBnlhxQGvExZ1mV27fWRQ,44581
-flywheel_bids/upload_bids.py,sha256=ZAdVX-3e4MA273h3BW6Kz0xKG_LQb7Sq_R7bLWmJKwc,57906
+flywheel_bids/templates/reproin.json,sha256=j9FxrVe3k6wqF4Kgice7FPnMDzdoDnVhkVD3FV0ylgU,44997
+flywheel_bids/upload_bids.py,sha256=Z8NkWpmTtOYGJkXx2gCWvvmkJ51X8DPnMIpIjgnodQ0,63150
 flywheel_bids/utils/download_run_level.py,sha256=NssyGcOzVURfagXgzryQHLA9YZFA3PzwI7EyuQUyVLU,13998
 flywheel_bids/utils/performance.py,sha256=rcC3DyvpdJO1k18vyFlK93OhazPctBwABb6qA4kNEMo,2026
 flywheel_bids/utils/run_level.py,sha256=FL_ChyzCECoUS7HrdRjWCEnE6e78wIA5ePKjQ4PiT2c,1816
 flywheel_bids/utils/tree.py,sha256=6bfQmkp1iqwk9RrIXxrMHMldwe9ALbvL81RA5NIqXn0,2522
 flywheel_bids/utils/validate.py,sha256=noMFXBD7_k8wioxhSIiw7JNzExiyPcS3DyBbpWUiaww,5898
-flywheel_bids-1.1.5.dist-info/METADATA,sha256=UGWatpfwqE1dYlU9wbCOZcTtZOxAZgY_KVPjPbIwS8E,5574
-flywheel_bids-1.1.5.dist-info/WHEEL,sha256=vVCvjcmxuUltf8cYhJ0sJMRDLr1XsPuxEId8YDzbyCY,88
-flywheel_bids-1.1.5.dist-info/LICENSE,sha256=dqr2l_pCmH5B2MnrF0BMMugS_MmWV4bPLqF9aHWn4Tk,1066
-flywheel_bids-1.1.5.dist-info/RECORD,,
+flywheel_bids-1.2.0.dist-info/METADATA,sha256=NnnzruihR01il-mp4lb4zcRCAkttvm6AEy5KNpuoQuA,5627
+flywheel_bids-1.2.0.dist-info/WHEEL,sha256=vVCvjcmxuUltf8cYhJ0sJMRDLr1XsPuxEId8YDzbyCY,88
+flywheel_bids-1.2.0.dist-info/LICENSE,sha256=dqr2l_pCmH5B2MnrF0BMMugS_MmWV4bPLqF9aHWn4Tk,1066
+flywheel_bids-1.2.0.dist-info/RECORD,,
```

